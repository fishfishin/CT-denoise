{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFOCIy67Mdlzunw7sdhbwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishfishin/CT-denoise/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NZFTkN1Yk6l"
      },
      "source": [
        "###!pip install visdom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5oOrOBAZGPQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27e01cf-52f6-4e8c-bb39-cd2edd22de11"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "#import visdom\n",
        "import torchvision.transforms as T\n",
        "import skimage.external.tifffile\n",
        "import copy\n",
        "import cv2\n",
        "import math\n",
        "from skimage import filters\n",
        "from torch.autograd import Variable\n",
        "from math import exp\n",
        "from skimage.restoration import denoise_nl_means\n",
        "from google.colab import drive\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\"\"\"\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "print(dev)\n",
        "#vis = visdom.Visdom()\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD5qj_ttAazJ"
      },
      "source": [
        "class doubleDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, output1,output2):\n",
        "        super(doubleDQN, self).__init__()\n",
        "\n",
        "        # image patch size is   h x w\n",
        "        #first two  shared conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=(2),padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=(2),padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # patch size after conv layer\n",
        "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
        "            return size // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(w))\n",
        "        convh = conv2d_size_out(conv2d_size_out(h))\n",
        "\n",
        "        linear_input_size = convw * convh * 64\n",
        "\n",
        "        #  Parameter selection\n",
        "        #  conv 64 Filter, kernel 3x3 \n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        #  FCL 128 neurons\n",
        "        self.dense1 = nn.Linear(linear_input_size, 128)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.head1 = nn.Linear(128,output1)\n",
        "       \n",
        "        linear_input_size = convw * convh  * 128\n",
        "        #  Parameter tuning\n",
        "        #  conv 128 Filter, kernel 3x3 \n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        #  conv 128 Filter, kernel 3x3 \n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "        #  FCL 256 neurons\n",
        "        self.dense2 = nn.Linear(linear_input_size, 256)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        self.head2 = nn.Linear(256, output2)\n",
        "\n",
        "        self.opt = optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        # 2 shared layers\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        #  Parameter selection\n",
        "        out1 = F.relu(self.bn3(self.conv3(x)))\n",
        "        out1 = out1.view(out1.size(0), -1)\n",
        "        out1 = F.relu(self.bn4(self.dense1(out1)))\n",
        "        out1 = self.head1(out1)\n",
        "       \n",
        "        #  Parameter tuning\n",
        "        out2 = F.relu(self.bn5(self.conv4(x)))\n",
        "        out2 = F.relu(self.bn6(self.conv5(out2)))\n",
        "        out2 = out2.view(out2.size(0), -1)\n",
        "        out2 = F.relu(self.bn7(self.dense2(out2)))\n",
        "        out2 = self.head2(out2)\n",
        "\n",
        "        return  out1, out2\n",
        "\n",
        "\n",
        "    def update(self, x,target1,target2):\n",
        "        #  inputs, labels = inputs.to(device), labels.to(device)\n",
        "        x = x.cuda()\n",
        "        out1,out2 = self.forward(x)\n",
        "        self.opt.zero_grad()\n",
        "        \n",
        "        loss1 = nn.MSELoss()\n",
        "        loss2 = nn.MSELoss()\n",
        "        upd_1 = loss1(out1,target1 )\n",
        "        upd_2 = loss2(out2,target2 )\n",
        "        upd_ = upd_1 + upd_2\n",
        "        upd_.backward()\n",
        "        \n",
        "        self.opt.step()\n",
        "\n",
        "        return upd_"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFyfDzO7ZQuK"
      },
      "source": [
        "\n",
        "\n",
        "####  1st path\n",
        "Para = [ 'patch_size']\n",
        "\n",
        "####  2nd path\n",
        "Actions = ['0.5','0.7','0.9','null','1.1','1.3','1.5']\n",
        "############ patch size from 1 to 7 for 9x9 patches\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "DISCOUNT_RATE = 0.99\n",
        "RESOLUTION = 256\n",
        "PATCH_SIZE = [9,9]\n",
        "Patch_num = RESOLUTION **2\n",
        "PATCH_reward = 5\n",
        "TARGET_UPDATE_STEP = 300\n",
        "MAXSTEPS_FILTER= 30\n",
        "REPLAY_MEMORY = 100000  ######### bufferï¼Ÿ\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "TUNING_STEP= 20\n",
        "\n",
        "def replay_train(mainDQN: doubleDQN, targetDQN: doubleDQN, states, next_states, action,parameter, rewards) -> np.float64:\n",
        "\n",
        "    X = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\n",
        "    X1 = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\n",
        "    X[:,0,:,:] = torch.from_numpy(states)\n",
        "    X1[:,0,:,:] = torch.from_numpy(next_states)\n",
        "\n",
        "    t1, t2 = targetDQN(Variable(X1).cuda())  #### old version of NET\n",
        "    y1, y2 = mainDQN(Variable(X).cuda())\n",
        "\n",
        "    ####  1st path\n",
        "    tem = torch.max(t1, axis=1)\n",
        "    temp = tem.values\n",
        "    Q_target1 = rewards + temp.cpu().detach().numpy() *DISCOUNT_RATE\n",
        "    for i in range(y1.shape[0]):\n",
        "        y1[i,int(parameter[i])] = Q_target1[i]\n",
        "\n",
        "    ####  2nd path\n",
        "    tem = torch.max(t2, axis=1)\n",
        "    temp = tem.values\n",
        "    Q_target2 = rewards + temp.cpu().detach().numpy() *DISCOUNT_RATE\n",
        "    for i in range(y2.shape[0]):\n",
        "        y2[i,int(action[i])] = Q_target2[i]\n",
        "\n",
        "    \n",
        "\n",
        "    return mainDQN.update(X, y1,y2)\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34-UqzF-Za8j"
      },
      "source": [
        "def PSNR(img1, img2):\n",
        "    D = np.array(img1 - img2, dtype=np.float64)\n",
        "    D[:, :] = D[:, :]**2\n",
        "    RMSE = D.sum()/img1.size\n",
        "    psnr = 10*math.log10(float(4095.0**2)/RMSE)\n",
        "    return psnr\n",
        "\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    #channel = 1\n",
        "    window = create_window(window_size, channel)\n",
        "    \n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    \n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cv3BPJ0Zu5E"
      },
      "source": [
        "def divide_patch(fimg) -> np.float64:\n",
        "\n",
        "    fimgpad = np.zeros((RESOLUTION + PATCH_SIZE[0] -1, RESOLUTION + PATCH_SIZE[0] -1),dtype =np.float64)\n",
        "    fimgpad[int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1,int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1]=fimg\n",
        "    state = np.zeros((Patch_num,PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    count = 0\n",
        "    for xcord in range(RESOLUTION):\n",
        "        for ycord in range(RESOLUTION):\n",
        "            temp=fimgpad[xcord:xcord+PATCH_SIZE[0],ycord:ycord+PATCH_SIZE[0]]\n",
        "            state[count,:,: ] = temp\n",
        "            count += 1\n",
        "    return state\n",
        "\n",
        "class NLM(nn.Module):\n",
        "    def __init__(self, p ):\n",
        "        super(NLM, self).__init__()\n",
        "        self.op = int(p)\n",
        "        self.s = 4\n",
        "        \n",
        "        self.p = self.op * 2\n",
        "        self.p +=1\n",
        "        self.h = torch.ones((1, 1, self.p, self.p), dtype=torch.float64)\n",
        "        self.w = self.s\n",
        "        self.s *= 2\n",
        "        self.m = nn.ReplicationPad2d(self.w)\n",
        "        self.c = 0.5 * torch.sum(self.h)\n",
        "\n",
        "    def forward(self, input):\n",
        " \n",
        "        x_stat = self.m(input)\n",
        "        z_stat = torch.zeros_like(x_stat,  dtype=torch.float64)\n",
        "        w_sum = torch.zeros_like(z_stat)\n",
        "\n",
        "        for i in range(int(self.s/2)):\n",
        "            for j in range(int(self.s/2)):\n",
        "                    \n",
        "                    m = nn.ReplicationPad2d((j,  self.s - j, i,  self.s - i))\n",
        "                    x_disp = m(input)\n",
        "                    u_n = torch.square(torch.sub(x_stat, x_disp))\n",
        "                    v_n = F.conv2d(u_n, self.h.cuda(), stride=1, padding=(self.op, self.op))       \n",
        "                    dist = torch.from_numpy(np.array((pow((self.w - i), 2) + pow((self.w - j), 2) )/2, dtype=np.float64))\n",
        "                    w_n = torch.exp( torch.add(-dist, torch.div(v_n, self.c)) )\n",
        "                    z_stat = torch.add(z_stat, torch.mul(x_disp, w_n))\n",
        "                    w_sum = torch.add(w_sum, w_n)\n",
        "\n",
        "        z_stat = torch.div(z_stat, w_sum)\n",
        "      \n",
        "        return z_stat[:,:, self.w:-self.w, self.w:-self.w]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLLD25fKZyhj"
      },
      "source": [
        "def Denoise(state, parameter, action, parameter_value, GroundTruth, original_image) -> np.float64:\n",
        "\n",
        "    current_patch = np.zeros((Patch_num, PATCH_SIZE[0], PATCH_SIZE[0]),dtype=np.float64)\n",
        "    org_patch = divide_patch(original_image)\n",
        "    # tuning parameter\n",
        "    #d = torch.zeros((1,1,9,9),dtype=torch.float64)\n",
        "    for idx in range(Patch_num):\n",
        "        if parameter[idx] ==0:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,0] = 1\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,0] = 2\n",
        "            if action[idx]==2:\n",
        "                parameter_value[idx,0] = 3\n",
        "            if action[idx]==3:\n",
        "                parameter_value[idx,0] = 4\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,0] = 5\n",
        "            if action[idx]==5:\n",
        "                parameter_value[idx,0] = 6\n",
        "            if action[idx]==6:\n",
        "                parameter_value[idx,0] = 7\n",
        "        \"\"\"\n",
        "        if parameter[idx]==1:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *1.5\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *1.3\n",
        "            if action[idx]==2:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1]*1.1\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *0.9\n",
        "            if action[idx]==5:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *0.7\n",
        "            if action[idx]==6:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *0.5\n",
        "            \n",
        "        \n",
        "        if parameter[idx]==2:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *1.5\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *1.1\n",
        "            if action[idx]==3:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1]*0.9\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *0.5\n",
        "        \"\"\"\n",
        "        #model = NLM(parameter_value[idx,0]).cuda()\n",
        "        #print(\" finish assignment\")\n",
        "        \n",
        "        #d[:,:,:,:] = torch.from_numpy(org_patch[idx,:,:])\n",
        "        final_patch = final_patch = denoise_nl_means(org_patch[idx,:,:], h =1.15*3,fast_mode=True,\n",
        "                                                        patch_size=parameter_value[idx,0],patch_distance=4,multichannel=False)\n",
        "        current_patch [idx, :, :] = final_patch\n",
        "        #print(\"patch: {}\".format(idx))\n",
        "    print(\"strat rewarding\")\n",
        "    ############# NLM\n",
        "\n",
        "    ### how to stitch  256x 256 patches into a single image fimg  after Bm3d\n",
        "    next_img = np.reshape(current_patch[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\n",
        "    next_state = current_patch\n",
        "    current_image = np.reshape(state[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\n",
        "    #############   calculate reward and error\n",
        "\n",
        "    dist1img = current_image - GroundTruth\n",
        "    dist2img = next_img - GroundTruth  #######################################\n",
        "    dist2 = np.reshape(dist2img, (Patch_num), order='A')\n",
        "\n",
        "    dist1imgLarge = np.zeros((RESOLUTION+PATCH_reward-1,RESOLUTION+PATCH_reward-1))\n",
        "    margin = int((PATCH_reward-1)/2)\n",
        "    dist1imgLarge[margin:RESOLUTION+margin,margin:RESOLUTION+margin]=np.absolute(dist1img)\n",
        "\n",
        "    dist2imgLarge = np.zeros((RESOLUTION + PATCH_reward-1, RESOLUTION + PATCH_reward-1))\n",
        "    dist2imgLarge[margin:RESOLUTION + margin, margin:RESOLUTION + margin] = np.absolute(dist2img)\n",
        "\n",
        "    rewardimg = np.zeros((RESOLUTION,RESOLUTION))\n",
        "    \n",
        "    for i in range(RESOLUTION):\n",
        "        for j in range(RESOLUTION):\n",
        "           \n",
        "            rewardimg[i,j]= 1/(np.sum(dist2imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.001) - 1/(np.sum(dist1imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.001)\n",
        "    reward = np.reshape(rewardimg,(Patch_num),order='A')\n",
        "    error = np.sum(np.absolute(dist2))\n",
        "\n",
        "\n",
        "    return next_state, reward, parameter_value, next_img ,error"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqkRdY-uadPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5842ce5-6a72-4a73-dee7-4de3aeb41670"
      },
      "source": [
        "def main():\n",
        "\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "    TrueImgTrain = skimage.external.tifffile.imread(\"drive/My Drive/Colab Notebooks/G.tif\")\n",
        "    TrueImgTrain = TrueImgTrain[0:2,:,:]\n",
        "    datasize = TrueImgTrain.shape\n",
        "\n",
        "    # (None, channel, H, w, depth) for Volume\n",
        "    mainDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1], len(Para),len(Actions)).cuda().double()\n",
        "    targetDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1], len(Para),len(Actions)).cuda().double()\n",
        "\n",
        "    state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    next_state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    action_sel = np.zeros((REPLAY_MEMORY))\n",
        "    reward_sel = np.zeros((REPLAY_MEMORY))\n",
        "    para_sel = np.zeros((REPLAY_MEMORY))\n",
        "    \n",
        "    indicator = 0 \n",
        "    if MAX_EPOCHS>0:  ########################## \n",
        "                   \n",
        "            State = np.zeros((datasize[0], Patch_num, PATCH_SIZE[0],PATCH_SIZE[0]))             ##  slabs  x  256**2 x 9 x 9\n",
        "            parameter_value = np.ones((Patch_num, len(Para)), dtype=np.float64)\n",
        "            \n",
        "            sigma = [4,5] #np.random.rand(2)*10\n",
        "            gaussian = np.random.normal(sigma[0], sigma[1], (datasize[0],datasize[1],datasize[2]))\n",
        "            train_data = TrueImgTrain + gaussian\n",
        "\n",
        "             \n",
        "            # for each image initializer\n",
        "            for IMG in range(datasize[0]):\n",
        "                state = divide_patch(  train_data[ IMG,:,:]  )  ##  256**2 x 9 x 9              \n",
        "                GroundTruth = TrueImgTrain[IMG,:, : ]      \n",
        "                ## initialize the 1st and 2nd paths \n",
        "                parameter = np.ones((Patch_num))\n",
        "                action = 2 * np.ones((Patch_num))\n",
        "                next_state, reward, parameter_value, img, error = Denoise( state,  parameter, action, parameter_value , GroundTruth, train_data[ IMG,:,:] )\n",
        "                State[ IMG, :, :, :] = next_state\n",
        "                \n",
        "                print(IMG)\n",
        "\n",
        "\n",
        "            State_initial = State\n",
        "            count_memory = 0\n",
        "\n",
        "            for episode in range(MAX_EPOCHS-1):\n",
        "\n",
        "                e = 0.999 / ((episode / 150) + 1)\n",
        "                if e<0.1:\n",
        "                    e=0.1\n",
        "                step_count = 0\n",
        "                State = State_initial\n",
        "\n",
        "                for ITER_NUM in range(MAXSTEPS_FILTER):\n",
        "\n",
        "                    for IMG_IDX in range(datasize[0]):\n",
        "                       \n",
        "                        state = State[ IMG_IDX, :, :, :]     ##  slabs x num of patches x 9 x 9\n",
        "                        GroundTruth = TrueImgTrain[IMG_IDX,:, : ]                      \n",
        "                        parameter = np.ones((Patch_num))\n",
        "                        action = 2 * np.ones((Patch_num))\n",
        "\n",
        "                        # random select patches and     action  for each image\n",
        "                        flag = np.random.rand(Patch_num)\n",
        "                        count_patch = 0\n",
        "                        length_patch = 0\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx]>=e:\n",
        "                                length_patch += 1\n",
        "                        \n",
        "                        # yy  : patch samples\n",
        "                        print(\"start sampling patches\")\n",
        "                        yy = torch.zeros(length_patch, 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx]<e:\n",
        "                                action[idx] = np.random.randint(len(Actions), size=1)\n",
        "                                parameter[idx] = np.random.randint(len(Para), size=1)\n",
        "                            if flag[idx]>=e:\n",
        "                                yy[count_patch,0,:, :] = torch.from_numpy(state[idx,:,:])\n",
        "                                count_patch += 1\n",
        "                        \n",
        "                        y1, y2 = mainDQN(Variable(yy).cuda())\n",
        "                        parameter_yy = torch.argmax(y1, axis=1)\n",
        "                        action_yy = torch.argmax(y2, axis=1)\n",
        "                     \n",
        "                        #### action and paramter chosen\n",
        "                        count_patch=0\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx] >= e:\n",
        "                                action[idx] = action_yy[count_patch]\n",
        "                                parameter[idx] = parameter_yy[count_patch]\n",
        "                                count_patch += 1\n",
        "                        print(\"strat denoise\")\n",
        "                        next_state, reward, parameter_value, img, error = Denoise( state,  parameter, action,parameter_value,  GroundTruth, train_data[ IMG_IDX,:,:] )\n",
        "                        psnr =PSNR(GroundTruth, img )\n",
        "                        \n",
        "                        img1 = torch.zeros(1, 1, datasize[1],datasize[2],dtype=torch.float64)\n",
        "                        img2 = torch.zeros(1, 1, datasize[1],datasize[2],dtype=torch.float64)\n",
        "                        img1[:,0,:,:] = torch.from_numpy(GroundTruth)\n",
        "                        img2[:,0,:,:] = torch.from_numpy(img)\n",
        "                        ssim_ = ssim(img1,img2)\n",
        "                        \n",
        "                        print(\" current PSNR : {} ssim : {} \".format(psnr,ssim_))\n",
        "                        name = str(step_count)\n",
        "                        #vis.image( img,opts='store_history')\n",
        "                        \n",
        "                        ###################  ? random replacement\n",
        "                        sel_prob = 0.01\n",
        "                        flag1 = np.random.rand(Patch_num)\n",
        "                        flag2 = np.zeros([Patch_num])\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag1[idx]>=sel_prob:\n",
        "                                flag2[idx] = 0\n",
        "                            if flag1[idx]<sel_prob:  ##### chosen\n",
        "                                flag2[idx] = 1\n",
        "\n",
        "                        sel_num = int(np.sum(flag2))\n",
        "\n",
        "                        ##### refresh the buffer randomly\n",
        "                        if count_memory+sel_num<=REPLAY_MEMORY-2:\n",
        "                            for idx in range(Patch_num):\n",
        "                                if flag1[idx]<sel_prob:\n",
        "                                    state_sel[count_memory,:,:] = state[idx,:,:]\n",
        "                                    next_state_sel[count_memory,:,:] = next_state[idx,:,:]\n",
        "                                    action_sel[count_memory]=action[idx]\n",
        "                                    para_sel[count_memory] = parameter[idx]\n",
        "                                    reward_sel[count_memory] = reward[idx]\n",
        "                                    #value_sel[count_memory] = parameter_value[idx,:]\n",
        "                                    \n",
        "                                    count_memory += 1\n",
        "                        else:\n",
        "                            indicator = 1\n",
        "                            for idx in range(Patch_num):\n",
        "                                if flag1[idx]<sel_prob:\n",
        "                                    state_sel[count_memory,:] = state[idx,:,:]\n",
        "                                    next_state_sel[count_memory,:] = next_state[idx,:,:]\n",
        "                                    action_sel[count_memory]=action[idx]\n",
        "                                    para_sel[count_memory] = parameter[idx]\n",
        "                                    reward_sel[count_memory] = reward[idx]\n",
        "                                    #value_sel[count_memory] = parameter_value[idx,:]\n",
        "                                    \n",
        "                                    if count_memory == REPLAY_MEMORY - 1:\n",
        "                                        count_memory = 0\n",
        "                                        print('Replay Memory is full')\n",
        "                                    else:\n",
        "                                        count_memory += 1\n",
        "                        if indicator == 0:\n",
        "                            replay_size = count_memory +  1\n",
        "                        else:\n",
        "                            replay_size = REPLAY_MEMORY\n",
        "\n",
        "                        if replay_size > BATCH_SIZE:\n",
        "\n",
        "                            print(\" tuning\")\n",
        "                            \n",
        "                            for i in range(TUNING_STEP):\n",
        "                                shuffle_order = np.arange(replay_size)\n",
        "                                np.random.shuffle(shuffle_order)\n",
        "                                minibatch_state = state_sel[shuffle_order[0:BATCH_SIZE], :, :]\n",
        "                                minibatch_next_state = next_state_sel[shuffle_order[0:BATCH_SIZE],:,:]\n",
        "                                minibatch_action = action_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                minibatch_parameter = para_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                minibatch_reward = reward_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                \n",
        "                                loss = replay_train(mainDQN, targetDQN, minibatch_state,minibatch_next_state,minibatch_action,minibatch_parameter, minibatch_reward)\n",
        "                                if step_count % TARGET_UPDATE_STEP == 0:\n",
        "                                    targetDQN = copy.deepcopy(mainDQN)\n",
        "                                step_count += 1\n",
        "                                #print(step_count)\n",
        "\n",
        "                        State[IMG_IDX, :, :, :] = next_state\n",
        "                        \n",
        "\n",
        "                    print(\"Episode: {}  Iterations: {} Loss: {}\".format(episode, ITER_NUM, loss))\n",
        "\n",
        "                CHECK = episode+1\n",
        "    # save the trained networks\n",
        "    PATH = \"drive/My Drive/Colab Notebooks/mainDQN\"\n",
        "    torch.save(mainDQN.state_dict(), PATH)\n",
        "    PATH = \"drive/My Drive/Colab Notebooks/targetDQN\"\n",
        "    torch.save(targetDQN.state_dict(), PATH)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "strat rewarding\n",
            "0\n",
            "strat rewarding\n",
            "1\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.853736718738766 ssim : 0.8603046337463554 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.7532088812076 ssim : 0.8555215376459882 \n",
            " tuning\n",
            "Episode: 0  Iterations: 0 Loss: 0.05402418958842211\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.81229786345638 ssim : 0.8608375549482208 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.97405325250488 ssim : 0.8569982907394811 \n",
            " tuning\n",
            "Episode: 0  Iterations: 1 Loss: 0.04343508589525587\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.8805461526715 ssim : 0.8593218816449264 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.759591157046685 ssim : 0.8575011571949199 \n",
            " tuning\n",
            "Episode: 0  Iterations: 2 Loss: 0.049983736031058865\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.81260608474943 ssim : 0.8599100845767415 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.87462660580447 ssim : 0.8560244494626416 \n",
            " tuning\n",
            "Episode: 0  Iterations: 3 Loss: 0.029812115737642884\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.757380016650664 ssim : 0.858073550879871 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.847442365965335 ssim : 0.8548787883785498 \n",
            " tuning\n",
            "Episode: 0  Iterations: 4 Loss: 0.02095349523919224\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.85948747803688 ssim : 0.8601882012091006 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.8045241381359 ssim : 0.8549090277524428 \n",
            " tuning\n",
            "Episode: 0  Iterations: 5 Loss: 0.02544782289102978\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.94985767928351 ssim : 0.8602730528143745 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.55583549540968 ssim : 0.8544654307092129 \n",
            " tuning\n",
            "Episode: 0  Iterations: 6 Loss: 0.02451224406179775\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.874095023784655 ssim : 0.8607604033342037 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.652344618890496 ssim : 0.8568446542543124 \n",
            " tuning\n",
            "Episode: 0  Iterations: 7 Loss: 0.007881791836568258\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.66809401528977 ssim : 0.8573615622217512 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.628249930584175 ssim : 0.8560073424814503 \n",
            " tuning\n",
            "Episode: 0  Iterations: 8 Loss: 0.014206536921188269\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.82977534702623 ssim : 0.8595948755037546 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.78161450085864 ssim : 0.8572712009329587 \n",
            " tuning\n",
            "Episode: 0  Iterations: 9 Loss: 0.008666567790956365\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.88064988898954 ssim : 0.8604256904906774 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.64332906984399 ssim : 0.8569903302804543 \n",
            " tuning\n",
            "Episode: 0  Iterations: 10 Loss: 0.005971735886761639\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.829797898230346 ssim : 0.8602398974358252 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.756243680528954 ssim : 0.8566192460353736 \n",
            " tuning\n",
            "Episode: 0  Iterations: 11 Loss: 0.007452918091300411\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.688397810300955 ssim : 0.8588723480210221 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.89657313389345 ssim : 0.8570036237129692 \n",
            " tuning\n",
            "Episode: 0  Iterations: 12 Loss: 0.0070318717940726135\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.4584249402252 ssim : 0.8588965363300273 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.0377347724996 ssim : 0.8586410201351107 \n",
            " tuning\n",
            "Episode: 0  Iterations: 13 Loss: 0.008564198246912335\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.872487399648 ssim : 0.8613586135675866 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.7598009807911 ssim : 0.861046896234146 \n",
            " tuning\n",
            "Episode: 0  Iterations: 14 Loss: 0.006428478425042525\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.68482763643199 ssim : 0.8570554997976153 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.481022652295216 ssim : 0.8559490847832322 \n",
            " tuning\n",
            "Episode: 0  Iterations: 15 Loss: 0.007229310767899496\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.60124606635028 ssim : 0.8583756108883321 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.655200392773224 ssim : 0.8583677035939545 \n",
            " tuning\n",
            "Episode: 0  Iterations: 16 Loss: 0.008463455788694755\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.77267579302574 ssim : 0.859683416695235 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.346174338686545 ssim : 0.8534332850323836 \n",
            " tuning\n",
            "Episode: 0  Iterations: 17 Loss: 0.004814936744339007\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.658743642186124 ssim : 0.8556963315218092 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.98257271390553 ssim : 0.856670866025514 \n",
            " tuning\n",
            "Episode: 0  Iterations: 18 Loss: 0.006017167775579216\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.98736458221433 ssim : 0.8616229006689882 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.844439193110794 ssim : 0.8556128739476454 \n",
            " tuning\n",
            "Episode: 0  Iterations: 19 Loss: 0.006912908594413295\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.67509330969408 ssim : 0.860745614346863 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.48897689209038 ssim : 0.8522998939548201 \n",
            " tuning\n",
            "Episode: 0  Iterations: 20 Loss: 0.014234233364888849\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.735497815756524 ssim : 0.8604145120858387 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.653867937114434 ssim : 0.8577397775649416 \n",
            " tuning\n",
            "Episode: 0  Iterations: 21 Loss: 0.004122556174783473\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.927496595586405 ssim : 0.8614980131443843 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.85422774446748 ssim : 0.855635763486023 \n",
            " tuning\n",
            "Episode: 0  Iterations: 22 Loss: 0.005112122027795105\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.79613543525919 ssim : 0.8582794215036644 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.708804674339596 ssim : 0.8567898089207742 \n",
            " tuning\n",
            "Episode: 0  Iterations: 23 Loss: 0.004976665884150296\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.73187075074496 ssim : 0.8591360493658433 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.71406768493696 ssim : 0.8567264505153976 \n",
            " tuning\n",
            "Episode: 0  Iterations: 24 Loss: 0.007080483075235271\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.767481806159246 ssim : 0.8603316407698995 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.532111759327265 ssim : 0.8552160687932598 \n",
            " tuning\n",
            "Episode: 0  Iterations: 25 Loss: 0.003135445661549589\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.76954604726266 ssim : 0.858397942204747 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.99540704904027 ssim : 0.8562795045839502 \n",
            " tuning\n",
            "Episode: 0  Iterations: 26 Loss: 0.004312039533837001\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.78478478528027 ssim : 0.8595133664741452 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.91063502073744 ssim : 0.8568098751367581 \n",
            " tuning\n",
            "Episode: 0  Iterations: 27 Loss: 0.003226004853753096\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.51073821441419 ssim : 0.860668601395675 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.49054966409422 ssim : 0.8550830264253475 \n",
            " tuning\n",
            "Episode: 0  Iterations: 28 Loss: 0.002768076818194072\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.65509167326861 ssim : 0.8591759166815093 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.940208668457 ssim : 0.8571449235448357 \n",
            " tuning\n",
            "Episode: 0  Iterations: 29 Loss: 0.003844844601176562\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.72805760136092 ssim : 0.8577048462458972 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.82402470895029 ssim : 0.8558003598642243 \n",
            " tuning\n",
            "Episode: 1  Iterations: 0 Loss: 0.006415868754700365\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.72437579036873 ssim : 0.8595944674014748 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.669999362982594 ssim : 0.8554701799381791 \n",
            " tuning\n",
            "Episode: 1  Iterations: 1 Loss: 0.0027529964133041185\n",
            "start sampling patches\n",
            "strat denoise\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}