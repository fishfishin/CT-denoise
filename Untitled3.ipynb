{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMoQ+OhbvHm+CCy+VDS/h1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishfishin/CT-denoise/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETnhgT-UIze_",
        "outputId": "52a129de-2e01-4d4f-f630-7f44700aae6e"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "#import visdom\n",
        "import torchvision.transforms as T\n",
        "import skimage.external.tifffile\n",
        "import copy\n",
        "import cv2\n",
        "import math\n",
        "from skimage import filters\n",
        "from torch.autograd import Variable\n",
        "from math import exp\n",
        "from skimage.restoration import denoise_nl_means\n",
        "from google.colab import drive\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\"\"\"\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "print(dev)\n",
        "#vis = visdom.Visdom()\n",
        "\n",
        "class doubleDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, output1,output2):\n",
        "        super(doubleDQN, self).__init__()\n",
        "\n",
        "        # image patch size is   h x w\n",
        "        #first two  shared conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=(2),padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=(2),padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # patch size after conv layer\n",
        "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
        "            return size // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(w))\n",
        "        convh = conv2d_size_out(conv2d_size_out(h))\n",
        "\n",
        "        linear_input_size = convw * convh * 64\n",
        "\n",
        "        #  Parameter selection\n",
        "        #  conv 64 Filter, kernel 3x3 \n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        #  FCL 128 neurons\n",
        "        self.dense1 = nn.Linear(linear_input_size, 128)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.head1 = nn.Linear(128,output1)\n",
        "       \n",
        "        linear_input_size = convw * convh  * 128\n",
        "        #  Parameter tuning\n",
        "        #  conv 128 Filter, kernel 3x3 \n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        #  conv 128 Filter, kernel 3x3 \n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "        #  FCL 256 neurons\n",
        "        self.dense2 = nn.Linear(linear_input_size, 256)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        self.head2 = nn.Linear(256, output2)\n",
        "\n",
        "        self.opt = optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        # 2 shared layers\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        #  Parameter selection\n",
        "        out1 = F.relu(self.bn3(self.conv3(x)))\n",
        "        out1 = out1.view(out1.size(0), -1)\n",
        "        out1 = F.relu(self.bn4(self.dense1(out1)))\n",
        "        out1 = self.head1(out1)\n",
        "       \n",
        "        #  Parameter tuning\n",
        "        out2 = F.relu(self.bn5(self.conv4(x)))\n",
        "        out2 = F.relu(self.bn6(self.conv5(out2)))\n",
        "        out2 = out2.view(out2.size(0), -1)\n",
        "        out2 = F.relu(self.bn7(self.dense2(out2)))\n",
        "        out2 = self.head2(out2)\n",
        "\n",
        "        return  out1, out2\n",
        "\n",
        "\n",
        "    def update(self, x,target1,target2):\n",
        "        #  inputs, labels = inputs.to(device), labels.to(device)\n",
        "        x = x.cuda()\n",
        "        out1,out2 = self.forward(x)\n",
        "        self.opt.zero_grad()\n",
        "        \n",
        "        loss1 = nn.MSELoss()\n",
        "        loss2 = nn.MSELoss()\n",
        "        upd_1 = loss1(out1,target1 )\n",
        "        upd_2 = loss2(out2,target2 )\n",
        "        upd_ = upd_1 + upd_2\n",
        "        upd_.backward()\n",
        "        \n",
        "        self.opt.step()\n",
        "\n",
        "        return upd_\n",
        "\n",
        "\n",
        "####  1st path\n",
        "Para = [ 'sigma']\n",
        "\n",
        "####  2nd path\n",
        "Actions = ['+1.5','1.1','null','0.9','-1.5']\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "DISCOUNT_RATE = 0.99\n",
        "RESOLUTION = 256\n",
        "PATCH_SIZE = [9,9]\n",
        "Patch_num = RESOLUTION **2\n",
        "PATCH_reward = 5\n",
        "TARGET_UPDATE_STEP = 300\n",
        "MAXSTEPS_FILTER= 30\n",
        "REPLAY_MEMORY = 100000  ######### bufferï¼Ÿ\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "TUNING_STEP= 20\n",
        "\n",
        "def replay_train(mainDQN: doubleDQN, targetDQN: doubleDQN, states, next_states, action,parameter, rewards) -> np.float64:\n",
        "\n",
        "    X = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\n",
        "    X1 = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\n",
        "    X[:,0,:,:] = torch.from_numpy(states)\n",
        "    X1[:,0,:,:] = torch.from_numpy(next_states)\n",
        "\n",
        "    t1, t2 = targetDQN(Variable(X1).cuda())  #### old version of NET\n",
        "    y1, y2 = mainDQN(Variable(X).cuda())\n",
        "\n",
        "    ####  1st path\n",
        "    tem = torch.max(t1, axis=1)\n",
        "    temp = tem.values\n",
        "    Q_target1 = rewards + temp.cpu().detach().numpy() *DISCOUNT_RATE\n",
        "    for i in range(y1.shape[0]):\n",
        "        y1[i,int(parameter[i])] = Q_target1[i]\n",
        "\n",
        "    ####  2nd path\n",
        "    tem = torch.max(t2, axis=1)\n",
        "    temp = tem.values\n",
        "    Q_target2 = rewards + temp.cpu().detach().numpy() *DISCOUNT_RATE\n",
        "    for i in range(y2.shape[0]):\n",
        "        y2[i,int(action[i])] = Q_target2[i]\n",
        "\n",
        "    \n",
        "\n",
        "    return mainDQN.update(X, y1,y2)\n",
        "\n",
        "def PSNR(img1, img2):\n",
        "    D = np.array(img1 - img2, dtype=np.float64)\n",
        "    D[:, :] = D[:, :]**2\n",
        "    RMSE = D.sum()/img1.size\n",
        "    psnr = 10*math.log10(float(1.0**2)/RMSE)\n",
        "    return psnr\n",
        "\n",
        "\n",
        "def gaussian(window_size, sigma):\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
        "    return gauss/gauss.sum()\n",
        "\n",
        "def create_window(window_size, channel):\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
        "    return window\n",
        "\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\n",
        "\n",
        "    mu1_sq = mu1.pow(2)\n",
        "    mu2_sq = mu2.pow(2)\n",
        "    mu1_mu2 = mu1*mu2\n",
        "\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\n",
        "\n",
        "    C1 = 0.01**2\n",
        "    C2 = 0.03**2\n",
        "\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "\n",
        "    if size_average:\n",
        "        return ssim_map.mean()\n",
        "    else:\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\n",
        "    (_, channel, _, _) = img1.size()\n",
        "    #channel = 1\n",
        "    window = create_window(window_size, channel)\n",
        "    \n",
        "    if img1.is_cuda:\n",
        "        window = window.cuda(img1.get_device())\n",
        "    window = window.type_as(img1)\n",
        "    \n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)\n",
        "\n",
        "def divide_patch(fimg) -> np.float64:\n",
        "\n",
        "    fimgpad = np.zeros((RESOLUTION + PATCH_SIZE[0] -1, RESOLUTION + PATCH_SIZE[0] -1),dtype =np.float64)\n",
        "    fimgpad[int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1,int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1]=fimg\n",
        "    state = np.zeros((Patch_num,PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    count = 0\n",
        "    for xcord in range(RESOLUTION):\n",
        "        for ycord in range(RESOLUTION):\n",
        "            temp=fimgpad[xcord:xcord+PATCH_SIZE[0],ycord:ycord+PATCH_SIZE[0]]\n",
        "            state[count,:,: ] = temp\n",
        "            count += 1\n",
        "    return state\n",
        "\n",
        "\n",
        "def Denoise(state, parameter, action, parameter_value, GroundTruth, original_image) -> np.float64:\n",
        "\n",
        "    current_patch = np.zeros((Patch_num, PATCH_SIZE[0], PATCH_SIZE[0]),dtype=np.float64)\n",
        "    org_patch = divide_patch(original_image)\n",
        "    # tuning parameter\n",
        "    #d = torch.zeros((1,1,9,9),dtype=torch.float64)\n",
        "    for idx in range(Patch_num):\n",
        "        if parameter[idx] ==0:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,0] = parameter_value[idx,0] *1.5\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,0] = parameter_value[idx,0] *1.1\n",
        "            if action[idx]==3:\n",
        "                parameter_value[idx,0] = parameter_value[idx,0]*0.9\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,0] = parameter_value[idx,0] *0.5\n",
        "          \n",
        "            \n",
        "        \"\"\"\n",
        "        if parameter[idx]==2:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *1.5\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *1.1\n",
        "            if action[idx]==3:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1]*0.9\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *0.5\n",
        "        \"\"\"\n",
        "        #model = NLM(parameter_value[idx,0], parameter_value[idx,1]).cuda()\n",
        "        #print(\" finish assignment\")\n",
        "        \n",
        "        #d[:,:,:,:] = torch.from_numpy(org_patch[idx,:,:])\n",
        "        final_patch = denoise_nl_means(org_patch[idx,:,:], h =1.15*parameter_value[idx,0],fast_mode=True,patch_size=1,patch_distance=4,multichannel=False)\n",
        "        current_patch [idx, :, :] = final_patch\n",
        "        #print(\"patch: {}\".format(idx))\n",
        "    print(\"strat rewarding\")\n",
        "    ############# NLM\n",
        "\n",
        "    ### how to stitch  256x 256 patches into a single image fimg  after Bm3d\n",
        "    next_img = np.reshape(current_patch[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\n",
        "    next_state = current_patch\n",
        "    current_image = np.reshape(state[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\n",
        "    #############   calculate reward and error\n",
        "\n",
        "    dist1img = current_image - GroundTruth\n",
        "    dist2img = next_img - GroundTruth  #######################################\n",
        "    dist2 = np.reshape(dist2img, (Patch_num), order='A')\n",
        "\n",
        "    dist1imgLarge = np.zeros((RESOLUTION+PATCH_reward-1,RESOLUTION+PATCH_reward-1))\n",
        "    margin = int((PATCH_reward-1)/2)\n",
        "    dist1imgLarge[margin:RESOLUTION+margin,margin:RESOLUTION+margin]=np.absolute(dist1img)\n",
        "\n",
        "    dist2imgLarge = np.zeros((RESOLUTION + PATCH_reward-1, RESOLUTION + PATCH_reward-1))\n",
        "    dist2imgLarge[margin:RESOLUTION + margin, margin:RESOLUTION + margin] = np.absolute(dist2img)\n",
        "\n",
        "    rewardimg = np.zeros((RESOLUTION,RESOLUTION))\n",
        "    \n",
        "    for i in range(RESOLUTION):\n",
        "        for j in range(RESOLUTION):\n",
        "           \n",
        "            rewardimg[i,j]= 1/(np.sum(dist2imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.001) - 1/(np.sum(dist1imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.001)\n",
        "    reward = np.reshape(rewardimg,(Patch_num),order='A')\n",
        "    error = np.sum(np.absolute(dist2))\n",
        "\n",
        "\n",
        "    return next_state, reward, parameter_value, next_img ,error\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "    TrueImgTrain = skimage.external.tifffile.imread(\"drive/My Drive/Colab Notebooks/G.tif\")/4095.0\n",
        "    TrueImgTrain = TrueImgTrain[0:2,:,:]\n",
        "    datasize = TrueImgTrain.shape\n",
        "\n",
        "    # (None, channel, H, w, depth) for Volume\n",
        "    mainDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1], len(Para),len(Actions)).cuda().double()\n",
        "    targetDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1], len(Para),len(Actions)).cuda().double()\n",
        "\n",
        "    state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    next_state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    action_sel = np.zeros((REPLAY_MEMORY))\n",
        "    reward_sel = np.zeros((REPLAY_MEMORY))\n",
        "    para_sel = np.zeros((REPLAY_MEMORY))\n",
        "    \n",
        "    indicator = 0 \n",
        "    if MAX_EPOCHS>0:  ########################## \n",
        "                   \n",
        "            State = np.zeros((datasize[0], Patch_num, PATCH_SIZE[0],PATCH_SIZE[0]))             ##  slabs  x  256**2 x 9 x 9\n",
        "            parameter_value = 0.99 *np.ones((Patch_num, len(Para)), dtype=np.float64)\n",
        "            \n",
        "            sigma = np.random.rand(2)*10\n",
        "            gaussian = np.random.normal(sigma[0], sigma[1], (datasize[0],datasize[1],datasize[2]))/4095.0\n",
        "            train_data = TrueImgTrain + gaussian\n",
        "\n",
        "             \n",
        "            # for each image initializer\n",
        "            for IMG in range(datasize[0]):\n",
        "                state = divide_patch(  train_data[ IMG,:,:]  )  ##  256**2 x 9 x 9              \n",
        "                GroundTruth = TrueImgTrain[IMG,:, : ]      \n",
        "                ## initialize the 1st and 2nd paths \n",
        "                parameter = np.ones((Patch_num))\n",
        "                action = 2 * np.ones((Patch_num))\n",
        "                next_state, reward, parameter_value, img, error = Denoise( state,  parameter, action, parameter_value , GroundTruth, train_data[ IMG,:,:] )\n",
        "                State[ IMG, :, :, :] = next_state\n",
        "                \n",
        "                print(IMG)\n",
        "\n",
        "\n",
        "            State_initial = State\n",
        "            count_memory = 0\n",
        "\n",
        "            for episode in range(MAX_EPOCHS-1):\n",
        "\n",
        "                e = 0.999 / ((episode / 150) + 1)\n",
        "                if e<0.1:\n",
        "                    e=0.1\n",
        "                step_count = 0\n",
        "                State = State_initial\n",
        "\n",
        "                for ITER_NUM in range(MAXSTEPS_FILTER):\n",
        "\n",
        "                    for IMG_IDX in range(datasize[0]):\n",
        "                       \n",
        "                        state = State[ IMG_IDX, :, :, :]     ##  slabs x num of patches x 9 x 9\n",
        "                        GroundTruth = TrueImgTrain[IMG_IDX,:, : ]                      \n",
        "                        parameter = np.ones((Patch_num))\n",
        "                        action = 2 * np.ones((Patch_num))\n",
        "\n",
        "                        # random select patches and     action  for each image\n",
        "                        flag = np.random.rand(Patch_num)\n",
        "                        count_patch = 0\n",
        "                        length_patch = 0\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx]>=e:\n",
        "                                length_patch += 1\n",
        "                        \n",
        "                        # yy  : patch samples\n",
        "                        print(\"start sampling patches\")\n",
        "                        yy = torch.zeros(length_patch, 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx]<e:\n",
        "                                action[idx] = np.random.randint(len(Actions), size=1)\n",
        "                                parameter[idx] = np.random.randint(len(Para), size=1)\n",
        "                            if flag[idx]>=e:\n",
        "                                yy[count_patch,0,:, :] = torch.from_numpy(state[idx,:,:])\n",
        "                                count_patch += 1\n",
        "                        \n",
        "                        y1, y2 = mainDQN(Variable(yy).cuda())\n",
        "                        parameter_yy = torch.argmax(y1, axis=1)\n",
        "                        action_yy = torch.argmax(y2, axis=1)\n",
        "                     \n",
        "                        #### action and paramter chosen\n",
        "                        count_patch=0\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx] >= e:\n",
        "                                action[idx] = action_yy[count_patch]\n",
        "                                parameter[idx] = parameter_yy[count_patch]\n",
        "                                count_patch += 1\n",
        "                        print(\"strat denoise\")\n",
        "                        next_state, reward, parameter_value, img, error = Denoise( state,  parameter, action,parameter_value,  GroundTruth, train_data[ IMG_IDX,:,:] )\n",
        "                        psnr =PSNR(GroundTruth, img )\n",
        "                        \n",
        "                        img1 = torch.zeros(1, 1, datasize[1],datasize[2],dtype=torch.float64)\n",
        "                        img2 = torch.zeros(1, 1, datasize[1],datasize[2],dtype=torch.float64)\n",
        "                        img1[:,0,:,:] = torch.from_numpy(GroundTruth)\n",
        "                        img2[:,0,:,:] = torch.from_numpy(img)\n",
        "                        ssim_ = ssim(img1,img2)\n",
        "                        \n",
        "                        print(\" current PSNR : {} ssim : {} \".format(psnr,ssim_))\n",
        "                        name = str(step_count)\n",
        "                        #vis.image( img,opts='store_history')\n",
        "                        \n",
        "                        ###################  ? random replacement\n",
        "                        sel_prob = 0.01\n",
        "                        flag1 = np.random.rand(Patch_num)\n",
        "                        flag2 = np.zeros([Patch_num])\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag1[idx]>=sel_prob:\n",
        "                                flag2[idx] = 0\n",
        "                            if flag1[idx]<sel_prob:  ##### chosen\n",
        "                                flag2[idx] = 1\n",
        "\n",
        "                        sel_num = int(np.sum(flag2))\n",
        "\n",
        "                        ##### refresh the buffer randomly\n",
        "                        if count_memory+sel_num<=REPLAY_MEMORY-2:\n",
        "                            for idx in range(Patch_num):\n",
        "                                if flag1[idx]<sel_prob:\n",
        "                                    state_sel[count_memory,:,:] = state[idx,:,:]\n",
        "                                    next_state_sel[count_memory,:,:] = next_state[idx,:,:]\n",
        "                                    action_sel[count_memory]=action[idx]\n",
        "                                    para_sel[count_memory] = parameter[idx]\n",
        "                                    reward_sel[count_memory] = reward[idx]\n",
        "                                    #value_sel[count_memory] = parameter_value[idx,:]\n",
        "                                    \n",
        "                                    count_memory += 1\n",
        "                        else:\n",
        "                            indicator = 1\n",
        "                            for idx in range(Patch_num):\n",
        "                                if flag1[idx]<sel_prob:\n",
        "                                    state_sel[count_memory,:] = state[idx,:,:]\n",
        "                                    next_state_sel[count_memory,:] = next_state[idx,:,:]\n",
        "                                    action_sel[count_memory]=action[idx]\n",
        "                                    para_sel[count_memory] = parameter[idx]\n",
        "                                    reward_sel[count_memory] = reward[idx]\n",
        "                                    #value_sel[count_memory] = parameter_value[idx,:]\n",
        "                                    \n",
        "                                    if count_memory == REPLAY_MEMORY - 1:\n",
        "                                        count_memory = 0\n",
        "                                        print('Replay Memory is full')\n",
        "                                    else:\n",
        "                                        count_memory += 1\n",
        "                        if indicator == 0:\n",
        "                            replay_size = count_memory +  1\n",
        "                        else:\n",
        "                            replay_size = REPLAY_MEMORY\n",
        "\n",
        "                        if replay_size > BATCH_SIZE:\n",
        "\n",
        "                            print(\" tuning\")\n",
        "                            \n",
        "                            for i in range(TUNING_STEP):\n",
        "                                shuffle_order = np.arange(replay_size)\n",
        "                                np.random.shuffle(shuffle_order)\n",
        "                                minibatch_state = state_sel[shuffle_order[0:BATCH_SIZE], :, :]\n",
        "                                minibatch_next_state = next_state_sel[shuffle_order[0:BATCH_SIZE],:,:]\n",
        "                                minibatch_action = action_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                minibatch_parameter = para_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                minibatch_reward = reward_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                \n",
        "                                loss = replay_train(mainDQN, targetDQN, minibatch_state,minibatch_next_state,minibatch_action,minibatch_parameter, minibatch_reward)\n",
        "                                if step_count % TARGET_UPDATE_STEP == 0:\n",
        "                                    targetDQN = copy.deepcopy(mainDQN)\n",
        "                                step_count += 1\n",
        "                                #print(step_count)\n",
        "\n",
        "                        State[IMG_IDX, :, :, :] = next_state\n",
        "                        \n",
        "\n",
        "                    print(\"Episode: {}  Iterations: {} Loss: {}\".format(episode, ITER_NUM, loss))\n",
        "\n",
        "                CHECK = episode+1\n",
        "    # save the trained networks\n",
        "    PATH = \"drive/My Drive/Colab Notebooks/mainDQN\"\n",
        "    torch.save(mainDQN.state_dict(), PATH)\n",
        "    PATH = \"drive/My Drive/Colab Notebooks/targetDQN\"\n",
        "    torch.save(targetDQN.state_dict(), PATH)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "strat rewarding\n",
            "0\n",
            "strat rewarding\n",
            "1\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.365366325284903 ssim : 0.7535948569987023 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.414966512165442 ssim : 0.7567217946592795 \n",
            " tuning\n",
            "Episode: 0  Iterations: 0 Loss: 0.006835399214905448\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.426476848057423 ssim : 0.7561401846921089 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.493139999074092 ssim : 0.760039005269771 \n",
            " tuning\n",
            "Episode: 0  Iterations: 1 Loss: 0.005508914374331703\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.51661248290913 ssim : 0.7601028004190151 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.604788620458713 ssim : 0.7647805852170846 \n",
            " tuning\n",
            "Episode: 0  Iterations: 2 Loss: 0.004994318714064952\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.644786253842746 ssim : 0.7655010964565382 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.746601520888614 ssim : 0.7708387333847386 \n",
            " tuning\n",
            "Episode: 0  Iterations: 3 Loss: 0.009739850029728759\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.787753317282807 ssim : 0.7718530661843723 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.88465654732919 ssim : 0.7769089736412056 \n",
            " tuning\n",
            "Episode: 0  Iterations: 4 Loss: 0.0059971073522124135\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 28.92943066858382 ssim : 0.7779445650883345 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.041831821282887 ssim : 0.7833824884402343 \n",
            " tuning\n",
            "Episode: 0  Iterations: 5 Loss: 0.010153015560207632\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.097661201974105 ssim : 0.7847364372228514 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.21009419530149 ssim : 0.7900775122331807 \n",
            " tuning\n",
            "Episode: 0  Iterations: 6 Loss: 0.010416557920800356\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.2411579756496 ssim : 0.7908365699114154 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.34660225887445 ssim : 0.7962082085783821 \n",
            " tuning\n",
            "Episode: 0  Iterations: 7 Loss: 0.003696965763044725\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.40864066405969 ssim : 0.7979688108586852 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.524485635298177 ssim : 0.8035122432594946 \n",
            " tuning\n",
            "Episode: 0  Iterations: 8 Loss: 0.0051517585568260135\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.5747502714893 ssim : 0.8045869734507463 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.702966569540653 ssim : 0.8106601044947874 \n",
            " tuning\n",
            "Episode: 0  Iterations: 9 Loss: 0.024433320492371742\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.7424305397817 ssim : 0.8110997137720524 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.842006768882648 ssim : 0.8159902002348008 \n",
            " tuning\n",
            "Episode: 0  Iterations: 10 Loss: 0.014641698483879077\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 29.881554991848837 ssim : 0.8167827619323429 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.01430297582988 ssim : 0.8227122055080757 \n",
            " tuning\n",
            "Episode: 0  Iterations: 11 Loss: 0.03161538117927751\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.069403835630077 ssim : 0.8236136891753691 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.190872823442323 ssim : 0.8295490922242603 \n",
            " tuning\n",
            "Episode: 0  Iterations: 12 Loss: 0.010975326722878383\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.23828979079875 ssim : 0.8303356799566699 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.365979327403014 ssim : 0.8356459181326094 \n",
            " tuning\n",
            "Episode: 0  Iterations: 13 Loss: 0.01390456079877566\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.417240843218508 ssim : 0.8366407379996628 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.53165746308744 ssim : 0.8413223351650044 \n",
            " tuning\n",
            "Episode: 0  Iterations: 14 Loss: 0.034324821416297745\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.563721109727698 ssim : 0.8414515313055235 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.69016350785855 ssim : 0.84662472347165 \n",
            " tuning\n",
            "Episode: 0  Iterations: 15 Loss: 0.03347514252142419\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.71843615750142 ssim : 0.8466732998021753 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.86474195967492 ssim : 0.8520013417001011 \n",
            " tuning\n",
            "Episode: 0  Iterations: 16 Loss: 0.028945816364874515\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 30.877997669117867 ssim : 0.8516671428129904 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.012955774391983 ssim : 0.8566398150656535 \n",
            " tuning\n",
            "Episode: 0  Iterations: 17 Loss: 0.021160967495186464\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.03094699265304 ssim : 0.8567059825313769 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.1700735184577 ssim : 0.8617899282443848 \n",
            " tuning\n",
            "Episode: 0  Iterations: 18 Loss: 0.03021302437892042\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.211450842631677 ssim : 0.8619986719862393 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.341943383118796 ssim : 0.8667231007703102 \n",
            " tuning\n",
            "Episode: 0  Iterations: 19 Loss: 0.017918030295107575\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.362497451732242 ssim : 0.8666042409380865 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.486277706191608 ssim : 0.8709688642402915 \n",
            " tuning\n",
            "Episode: 0  Iterations: 20 Loss: 0.03775663518519508\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.511462667626144 ssim : 0.8712725833733908 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.659005572748402 ssim : 0.8758906518839219 \n",
            " tuning\n",
            "Episode: 0  Iterations: 21 Loss: 0.03421923166130759\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.705969652793527 ssim : 0.8762536232284146 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.834034485536556 ssim : 0.8804120343033133 \n",
            " tuning\n",
            "Episode: 0  Iterations: 22 Loss: 0.06993962024684991\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.861265387259973 ssim : 0.8803185532926147 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 31.99754884953613 ssim : 0.8841391456875778 \n",
            " tuning\n",
            "Episode: 0  Iterations: 23 Loss: 0.05384105996716009\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.01726007124874 ssim : 0.8842328461876675 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.16852197172833 ssim : 0.8885243805346348 \n",
            " tuning\n",
            "Episode: 0  Iterations: 24 Loss: 0.07766121036383564\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.216552687083905 ssim : 0.8889129083947049 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.350983985218676 ssim : 0.8927066456102795 \n",
            " tuning\n",
            "Episode: 0  Iterations: 25 Loss: 0.04948715630027142\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.39115256054396 ssim : 0.893232007251847 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.539699113277784 ssim : 0.8967965727809014 \n",
            " tuning\n",
            "Episode: 0  Iterations: 26 Loss: 0.04562694150189668\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.57770401705474 ssim : 0.8970033184420207 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.711796615595674 ssim : 0.9005902219859688 \n",
            " tuning\n",
            "Episode: 0  Iterations: 27 Loss: 0.05565434186011022\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.73557102962545 ssim : 0.9004905108296315 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.851135248907745 ssim : 0.9037644345575936 \n",
            " tuning\n",
            "Episode: 0  Iterations: 28 Loss: 0.20685785454073335\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 32.88098884536812 ssim : 0.9038095063537773 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.029921862067695 ssim : 0.9072424335281521 \n",
            " tuning\n",
            "Episode: 0  Iterations: 29 Loss: 0.07565138384161346\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.081902684307636 ssim : 0.9079276734132863 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.18058453289569 ssim : 0.9104340031203741 \n",
            " tuning\n",
            "Episode: 1  Iterations: 0 Loss: 0.06316093213046368\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.19545132886914 ssim : 0.9107436869775924 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.32353931291135 ssim : 0.9135448370069574 \n",
            " tuning\n",
            "Episode: 1  Iterations: 1 Loss: 0.1382325543742363\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.334455977673805 ssim : 0.9136340568090273 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.484063033765125 ssim : 0.9163779086013696 \n",
            " tuning\n",
            "Episode: 1  Iterations: 2 Loss: 0.07362327244292262\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.5014249917284 ssim : 0.916334331400987 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.64960318863568 ssim : 0.919128779021732 \n",
            " tuning\n",
            "Episode: 1  Iterations: 3 Loss: 0.11965979194831482\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.698423264366184 ssim : 0.9199199652056803 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.85505105958123 ssim : 0.9226976787448009 \n",
            " tuning\n",
            "Episode: 1  Iterations: 4 Loss: 0.14047869284370612\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 33.89031791472544 ssim : 0.9232074919639459 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.034845618630996 ssim : 0.9258152028781632 \n",
            " tuning\n",
            "Episode: 1  Iterations: 5 Loss: 0.15346164514417548\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.08218278506038 ssim : 0.926199632978179 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.19092795639935 ssim : 0.9279759097950778 \n",
            " tuning\n",
            "Episode: 1  Iterations: 6 Loss: 0.15697820453436812\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.21725051728932 ssim : 0.9284288735402559 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.360069874238995 ssim : 0.9307424563166575 \n",
            " tuning\n",
            "Episode: 1  Iterations: 7 Loss: 0.2601811282165836\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.41388055637493 ssim : 0.9313369880068602 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.55136278511254 ssim : 0.9334150086857441 \n",
            " tuning\n",
            "Episode: 1  Iterations: 8 Loss: 0.08302237516739952\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.56834489639565 ssim : 0.933527996773672 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.69616315965631 ssim : 0.9355439813111943 \n",
            " tuning\n",
            "Episode: 1  Iterations: 9 Loss: 0.15935582325478675\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.75133169840372 ssim : 0.9363402189023464 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.871238835833736 ssim : 0.9379716213456308 \n",
            " tuning\n",
            "Episode: 1  Iterations: 10 Loss: 0.2057924511084613\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 34.89236021052561 ssim : 0.9384234042404417 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.06382783063485 ssim : 0.9404023035965237 \n",
            " tuning\n",
            "Episode: 1  Iterations: 11 Loss: 0.24020733317611884\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.097807118107234 ssim : 0.9410432657824802 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.20337734514134 ssim : 0.9424578460573326 \n",
            " tuning\n",
            "Episode: 1  Iterations: 12 Loss: 0.30237369461993835\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.27581776047853 ssim : 0.9431265707940156 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.39053444006653 ssim : 0.9447391261287341 \n",
            " tuning\n",
            "Episode: 1  Iterations: 13 Loss: 0.30759712133010425\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.41495817456329 ssim : 0.9450169375450242 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.49836278676656 ssim : 0.946433628666183 \n",
            " tuning\n",
            "Episode: 1  Iterations: 14 Loss: 0.1516657822090035\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.54150498315145 ssim : 0.9470219937279064 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.62451073893565 ssim : 0.947998359789447 \n",
            " tuning\n",
            "Episode: 1  Iterations: 15 Loss: 0.25571695311627596\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.699777877766515 ssim : 0.9487707079209718 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.844836221546984 ssim : 0.9501711837209899 \n",
            " tuning\n",
            "Episode: 1  Iterations: 16 Loss: 0.2656272911972779\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 35.88050194889413 ssim : 0.9507127292439711 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.00071887916804 ssim : 0.9520330525220553 \n",
            " tuning\n",
            "Episode: 1  Iterations: 17 Loss: 0.6856435606916929\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.02192048611295 ssim : 0.9521585497938436 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.14405690664211 ssim : 0.9534527575924598 \n",
            " tuning\n",
            "Episode: 1  Iterations: 18 Loss: 0.22582940014833688\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.17008482424176 ssim : 0.9536106174139017 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.34894243973234 ssim : 0.9552525356319065 \n",
            " tuning\n",
            "Episode: 1  Iterations: 19 Loss: 0.19016420476598583\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.324128254432225 ssim : 0.9552294042637264 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.46362897348081 ssim : 0.956661543202491 \n",
            " tuning\n",
            "Episode: 1  Iterations: 20 Loss: 0.23729804205656546\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.46126186407789 ssim : 0.9567203128807391 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.601957693832006 ssim : 0.9579501718352909 \n",
            " tuning\n",
            "Episode: 1  Iterations: 21 Loss: 0.4654963653918305\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.585679000804106 ssim : 0.9580407262419298 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.71360238996378 ssim : 0.9591861261186121 \n",
            " tuning\n",
            "Episode: 1  Iterations: 22 Loss: 0.37618606509392666\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.75858010742112 ssim : 0.9596384576877486 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.895317676080715 ssim : 0.9607544508146355 \n",
            " tuning\n",
            "Episode: 1  Iterations: 23 Loss: 0.5581894852377203\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 36.97590346927599 ssim : 0.9614987049308695 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.14236270961702 ssim : 0.9623521564339506 \n",
            " tuning\n",
            "Episode: 1  Iterations: 24 Loss: 0.21026685067049464\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.19026758867943 ssim : 0.9627757238067546 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.29363395444264 ssim : 0.963537740865811 \n",
            " tuning\n",
            "Episode: 1  Iterations: 25 Loss: 0.42588255286278276\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.290269381254106 ssim : 0.9638171045522734 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.45694455071654 ssim : 0.964808530275156 \n",
            " tuning\n",
            "Episode: 1  Iterations: 26 Loss: 0.35480710638120816\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.48972234482499 ssim : 0.9652757201765085 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.62273671837264 ssim : 0.9661092780679251 \n",
            " tuning\n",
            "Episode: 1  Iterations: 27 Loss: 0.6856646365634724\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.60121714832747 ssim : 0.9662201681679277 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.695868484305365 ssim : 0.9669656359992205 \n",
            " tuning\n",
            "Episode: 1  Iterations: 28 Loss: 0.34283008067656595\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.81104933413201 ssim : 0.9675541910587236 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.86976844097389 ssim : 0.9680656559796392 \n",
            " tuning\n",
            "Episode: 1  Iterations: 29 Loss: 0.5106963263218283\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.8780379514657 ssim : 0.9683808076486264 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 37.96421946574915 ssim : 0.9688214210800199 \n",
            " tuning\n",
            "Episode: 2  Iterations: 0 Loss: 0.3398297584913096\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.07868300190991 ssim : 0.969459264189959 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.22200676496876 ssim : 0.9698029176418987 \n",
            " tuning\n",
            "Episode: 2  Iterations: 1 Loss: 0.9089426800273902\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.25833735350899 ssim : 0.9705804587920356 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.30842480001641 ssim : 0.9708219478545296 \n",
            " tuning\n",
            "Episode: 2  Iterations: 2 Loss: 0.3858567607631447\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.34248590817522 ssim : 0.9712178380411605 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.48872742366914 ssim : 0.9717043231536187 \n",
            " tuning\n",
            "Episode: 2  Iterations: 3 Loss: 0.6215193904533447\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.58483966034443 ssim : 0.9723998605816142 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.670425701860474 ssim : 0.9727890992181316 \n",
            " tuning\n",
            "Episode: 2  Iterations: 4 Loss: 0.44497080381964316\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.70250632049762 ssim : 0.9732429453755351 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.87165952129664 ssim : 0.9739296779776091 \n",
            " tuning\n",
            "Episode: 2  Iterations: 5 Loss: 0.5573794908749207\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 38.97119184766994 ssim : 0.9745783436579765 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.09622772147101 ssim : 0.9751342348083009 \n",
            " tuning\n",
            "Episode: 2  Iterations: 6 Loss: 0.3689372342935545\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.206873767086044 ssim : 0.9756086879249231 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.33507838846494 ssim : 0.9760109635315317 \n",
            " tuning\n",
            "Episode: 2  Iterations: 7 Loss: 0.727801031679149\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.36004232005488 ssim : 0.9762444032997506 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.46606065662449 ssim : 0.9766810646903071 \n",
            " tuning\n",
            "Episode: 2  Iterations: 8 Loss: 0.3730030730470883\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.579160717388646 ssim : 0.9771195934254719 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.71760424929872 ssim : 0.9778729706548532 \n",
            " tuning\n",
            "Episode: 2  Iterations: 9 Loss: 0.745045908969816\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.69577384962014 ssim : 0.9778772508762744 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.84357914907255 ssim : 0.9784985507281004 \n",
            " tuning\n",
            "Episode: 2  Iterations: 10 Loss: 1.4804389981983128\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 39.86317435148223 ssim : 0.9785464252993095 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.03491123364068 ssim : 0.9792561648220492 \n",
            " tuning\n",
            "Episode: 2  Iterations: 11 Loss: 0.5206299295027035\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.00210240880777 ssim : 0.979196167232804 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.11307289895744 ssim : 0.9798553920509276 \n",
            " tuning\n",
            "Episode: 2  Iterations: 12 Loss: 1.1998415517891843\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.14445418468205 ssim : 0.9798499840567314 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.32845638211748 ssim : 0.9805106223335006 \n",
            " tuning\n",
            "Episode: 2  Iterations: 13 Loss: 0.9790293418154905\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.38761858643448 ssim : 0.9805866417800514 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.47246476898068 ssim : 0.981100381616374 \n",
            " tuning\n",
            "Episode: 2  Iterations: 14 Loss: 1.0605764453624646\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.55787220144452 ssim : 0.9812520583868707 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.71327267862663 ssim : 0.9817971689052107 \n",
            " tuning\n",
            "Episode: 2  Iterations: 15 Loss: 0.5880471854787628\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.71423097763141 ssim : 0.9817909175198709 \n",
            "Replay Memory is full\n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.82434905062104 ssim : 0.9822542724910691 \n",
            " tuning\n",
            "Episode: 2  Iterations: 16 Loss: 0.615151467513562\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.95944061869287 ssim : 0.9825251402012523 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 40.991647852242394 ssim : 0.9828287440796052 \n",
            " tuning\n",
            "Episode: 2  Iterations: 17 Loss: 0.7382470118256843\n",
            "start sampling patches\n",
            "strat denoise\n",
            "strat rewarding\n",
            " current PSNR : 41.085085006592934 ssim : 0.9830234192521792 \n",
            " tuning\n",
            "start sampling patches\n",
            "strat denoise\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}