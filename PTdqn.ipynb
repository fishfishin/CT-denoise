{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PTdqn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPN1paQUrjda3vuyNUQ2v9L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishfishin/CT-denoise/blob/main/PTdqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZUMqysI4Vps"
      },
      "source": [
        "import math\r\n",
        "import random\r\n",
        "#import pydicom\r\n",
        "import glob\r\n",
        "import numpy as np\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count\r\n",
        "from PIL import Image\r\n",
        "import torch\r\n",
        "import torchvision\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "#import visdom\r\n",
        "import torchvision.transforms as T\r\n",
        "import skimage.external.tifffile\r\n",
        "import copy\r\n",
        "import cv2\r\n",
        "import math\r\n",
        "from skimage import filters\r\n",
        "from torch.autograd import Variable\r\n",
        "from math import exp\r\n",
        "from skimage.transform import resize\r\n",
        "from skimage.transform import radon,iradon\r\n",
        "from skimage.restoration import denoise_nl_means\r\n",
        "#from google.colab import drive\r\n",
        "\"\"\"\r\n",
        "import tensorflow as tf\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "if device_name != '/device:GPU:0':\r\n",
        "  raise SystemError('GPU device not found')\r\n",
        "print('Found GPU at: {}'.format(device_name))\r\n",
        "\"\"\"\r\n",
        "if torch.cuda.is_available():  \r\n",
        "  dev = \"cuda:0\" \r\n",
        "else:  \r\n",
        "  dev = \"cpu\"  \r\n",
        "print(dev)\r\n",
        "#vis = visdom.Visdom()\r\n",
        "\r\n",
        "def huber_loss_simple(a, b):\r\n",
        "    error = a - b\r\n",
        "    if abs(error) > 1.0:\r\n",
        "        return abs(error) - 1/2\r\n",
        "    return error*error / 2\r\n",
        "\r\n",
        "class doubleDQN(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, h, w,output):\r\n",
        "        super(doubleDQN, self).__init__()\r\n",
        "\r\n",
        "        # image patch size is   h x w\r\n",
        "        #first two  shared conv layersoutput1,\r\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=(2),padding=1)\r\n",
        "        self.bn1 = nn.BatchNorm2d(64)\r\n",
        "        #self.convs = nn.Conv2d(16, 32, kernel_size=3, stride=(2),padding=1)\r\n",
        "        #self.bns = nn.BatchNorm2d(32)\r\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=(2),padding=1)\r\n",
        "        self.bn2 = nn.BatchNorm2d(64)\r\n",
        "\r\n",
        "        # patch size after conv layerconv2d_size_out( conv2d_size_out(\r\n",
        "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\r\n",
        "            return size // stride  + 1\r\n",
        "        convw =conv2d_size_out(w)\r\n",
        "        convh = conv2d_size_out(h)\r\n",
        "\r\n",
        "        \r\n",
        "        \r\n",
        "        linear_input_size = convw * convh  * 128\r\n",
        "        #  Parameter tuning\r\n",
        "        #  conv 128 Filter, kernel 3x3 \r\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1,padding=1)\r\n",
        "        self.bn5 = nn.BatchNorm2d(128)\r\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1)\r\n",
        "        self.bn6 = nn.BatchNorm2d(128)\r\n",
        "        #  FCL 256 neurons\r\n",
        "        self.dense2 = nn.Linear(linear_input_size, 128)\r\n",
        "        self.bn7 = nn.BatchNorm1d(128)\r\n",
        "        self.head2 = nn.Linear(128, output)\r\n",
        "\r\n",
        "        self.opt = optim.SGD(self.parameters(), lr=0.001, momentum=0.99)\r\n",
        "\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "\r\n",
        "\r\n",
        "        # 2 shared layers\r\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\r\n",
        "        #x = F.relu(self.bns(self.convs(x)))\r\n",
        "        #x = F.relu(self.bn2(self.conv2(x)))\r\n",
        "         \r\n",
        "        \r\n",
        "        \r\n",
        "        #  Parameter tuning\r\n",
        "        out2 = F.relu(self.bn5(self.conv4(x)))\r\n",
        "        out2 = F.relu(self.bn6(self.conv5(out2)))\r\n",
        "        out2 = out2.view(out2.size(0), -1)\r\n",
        "        out2 = F.relu(self.bn7(self.dense2(out2)))\r\n",
        "        out2 = self.head2(out2)\r\n",
        "\r\n",
        "        return  out2\r\n",
        "\r\n",
        "\r\n",
        "    def update(self, x,target):\r\n",
        "        #  inputs, labels = inputs.to(device), labels.to(device)\r\n",
        "        x = x.cuda()\r\n",
        "        out = self.forward(x)\r\n",
        "        self.opt.zero_grad()\r\n",
        "        \r\n",
        "        loss1 = nn.SmoothL1Loss()\r\n",
        "        upd_1 = loss1(out,target )\r\n",
        "        \r\n",
        "        upd_ =  upd_1 \r\n",
        "        upd_.backward()\r\n",
        "        \r\n",
        "        self.opt.step()\r\n",
        "\r\n",
        "        return upd_\r\n",
        "\r\n",
        "\r\n",
        "####  1st path'patch size','searchsize',\r\n",
        "Para = [ 'h']\r\n",
        "\r\n",
        "####  2nd path\r\n",
        "Actions = ['+1.5','1.1','null','0.9','-1.5']\r\n",
        "\r\n",
        "MAX_EPOCHS = 50\r\n",
        "DISCOUNT_RATE = 0.99\r\n",
        "RESOLUTION = 256\r\n",
        "PATCH_SIZE = [5,5]\r\n",
        "Patch_num = RESOLUTION**2\r\n",
        "PATCH_reward = 5           ### ssim and psnr\r\n",
        "TARGET_UPDATE_STEP = 300\r\n",
        "MAXSTEPS_FILTER=21   ####   21   if large then will get stuck in certain pattern\r\n",
        "REPLAY_MEMORY = 4000000  ######### bufferï¼Ÿ\r\n",
        "BATCH_SIZE = 512\r\n",
        "\r\n",
        "TUNING_STEP= 20\r\n",
        "Nsample=2000\r\n",
        "def replay_train(mainDQN: doubleDQN, targetDQN: doubleDQN, states, next_states, action, rewards) -> np.float64:\r\n",
        "\r\n",
        "    X = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\r\n",
        "    X1 = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\r\n",
        "    X[:,0,:,:] = torch.from_numpy(states)\r\n",
        "    X1[:,0,:,:] = torch.from_numpy(next_states)\r\n",
        "\r\n",
        "    t = targetDQN(Variable(X1).cuda())  ## old version of NET\r\n",
        "    y = mainDQN(Variable(X).cuda())\r\n",
        "     \r\n",
        " \r\n",
        "    ####  2nd path\r\n",
        "    tem = torch.max(t, axis=1)\r\n",
        "    temp = tem.values\r\n",
        "    Q_target = rewards + temp.cpu().detach().numpy() *DISCOUNT_RATE\r\n",
        "    for i in range(y.shape[0]):\r\n",
        "        y[i,int(action[i])] = Q_target[i]\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "    return mainDQN.update(X,y)\r\n",
        "\r\n",
        "\r\n",
        "def PSNR(img1, img2):\r\n",
        "    D = np.array(img1 - img2, dtype=np.float64)\r\n",
        "    D[:, :] = D[:, :]**2\r\n",
        "    RMSE = D.sum()/img1.size\r\n",
        "    psnr = 10*math.log10(float(1.0**2)/RMSE)\r\n",
        "    return psnr\r\n",
        "\r\n",
        "\r\n",
        "def gaussian(window_size, sigma):\r\n",
        "    gauss = torch.Tensor([exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\r\n",
        "    return gauss/gauss.sum()\r\n",
        "\r\n",
        "def create_window(window_size, channel):\r\n",
        "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\r\n",
        "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\r\n",
        "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\r\n",
        "    return window\r\n",
        "\r\n",
        "def _ssim(img1, img2, window, window_size, channel, size_average = True):\r\n",
        "    mu1 = F.conv2d(img1, window, padding = window_size//2, groups = channel)\r\n",
        "    mu2 = F.conv2d(img2, window, padding = window_size//2, groups = channel)\r\n",
        "\r\n",
        "    mu1_sq = mu1.pow(2)\r\n",
        "    mu2_sq = mu2.pow(2)\r\n",
        "    mu1_mu2 = mu1*mu2\r\n",
        "\r\n",
        "    sigma1_sq = F.conv2d(img1*img1, window, padding = window_size//2, groups = channel) - mu1_sq\r\n",
        "    sigma2_sq = F.conv2d(img2*img2, window, padding = window_size//2, groups = channel) - mu2_sq\r\n",
        "    sigma12 = F.conv2d(img1*img2, window, padding = window_size//2, groups = channel) - mu1_mu2\r\n",
        "\r\n",
        "    C1 = 0.01**2\r\n",
        "    C2 = 0.03**2\r\n",
        "\r\n",
        "    ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2))/((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\r\n",
        "\r\n",
        "    if size_average:\r\n",
        "        return ssim_map.mean()\r\n",
        "    else:\r\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\r\n",
        "\r\n",
        "\r\n",
        "def ssim(img1, img2, window_size = 11, size_average = True):\r\n",
        "    (_, channel, _, _) = img1.size()\r\n",
        "    #channel = 1\r\n",
        "    window = create_window(window_size, channel)\r\n",
        "    \r\n",
        "    if img1.is_cuda:\r\n",
        "        window = window.cuda(img1.get_device())\r\n",
        "    window = window.type_as(img1)\r\n",
        "    \r\n",
        "    return _ssim(img1, img2, window, window_size, channel, size_average)\r\n",
        "\r\n",
        "def divide_patch(fimg) -> np.float64:\r\n",
        "\r\n",
        "    fimgpad = np.zeros((RESOLUTION + PATCH_SIZE[0] -1, RESOLUTION + PATCH_SIZE[0] -1),dtype =np.float64)\r\n",
        "    fimgpad[int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1,int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1]=fimg\r\n",
        "    state = np.zeros((Patch_num,PATCH_SIZE[0], PATCH_SIZE[0]))\r\n",
        "    count = 0\r\n",
        "    for xcord in range(RESOLUTION):\r\n",
        "        for ycord in range(RESOLUTION):\r\n",
        "            temp=fimgpad[xcord:xcord+PATCH_SIZE[0],ycord:ycord+PATCH_SIZE[0]]\r\n",
        "            state[count,:,: ] = temp\r\n",
        "            count += 1\r\n",
        "    return state\r\n",
        "\r\n",
        "\r\n",
        "def Denoise(state, action, parameter_value, GroundTruth, org_patch) -> np.float64:\r\n",
        "\r\n",
        "    next_patch = np.zeros((Patch_num, PATCH_SIZE[0], PATCH_SIZE[0]),dtype=np.float64)\r\n",
        "    #org_patch = _patch(original_image)\r\n",
        "    #G = divide_patch(GroundTruth)\r\n",
        "    # tuning parameter\r\n",
        "    #d = torch.zeros((1,1,9,9),dtype=torch.float64)\r\n",
        "    #current_image = np.reshape(state[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\r\n",
        "    # reward = np.zeros(Patch_num)\r\n",
        "    \"\"\"\r\n",
        "    S = torch.zeros(1, 1,RESOLUTION,RESOLUTION,dtype=torch.float64)\r\n",
        "    S[:,:,:,:] = torch.from_numpy(current_image)\r\n",
        "    rs = Q.forward(S)\r\n",
        "    \"\"\"\r\n",
        "    for idx in range(Patch_num):\r\n",
        "        #temp =S\r\n",
        "        if 0==0: \r\n",
        "            if action[idx]==0:\r\n",
        "                parameter_value[idx,0] *=  1.5\r\n",
        "            if action[idx]==1:\r\n",
        "                parameter_value[idx,0] *= 1.1\r\n",
        "            if action[idx]==3:\r\n",
        "                parameter_value[idx,0] *= 0.9 \r\n",
        "            if action[idx]==4:\r\n",
        "                parameter_value[idx,0] *=  0.5 \r\n",
        "        \r\n",
        "                \r\n",
        "        next_patch[idx, :, :] = denoise_nl_means(org_patch[idx,:,:],h= parameter_value[idx,0],fast_mode=True,\r\n",
        "                                            patch_size=2,patch_distance=1,multichannel=False)  \r\n",
        "        #reward[idx] = 1.0 / (np.sqrt(np.sum((G[idx, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2 ] - final_patch[int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2])**2)) +0.00001) \r\n",
        "        # -1.0 / (np.sqrt(np.sum(G[idx, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2] - state[idx, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2])**2))+0.00001)\r\n",
        "        \"\"\"\r\n",
        "            a = idx/RESOLUTION\r\n",
        "            b = idx%RESOLUTION\r\n",
        "            #temp[:, :,a:a+PATCH_SIZE[0], b:b+PATCH_SIZE[0]] = torch.from_numpy(final_patch)  \r\n",
        "            temp[:, :,a, b]=  final_patch[int(PATCH_SIZE[0]//2),int(PATCH_SIZE[0]//2)]   \r\n",
        "            reward[idx] = Q.forward(temp) - rs\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        #reward[idx] = -np.sqrt(np.sum((G[idx, :, :] - final_patch)**2)) + np.sqrt(np.sum((G[idx, :, :] - state[idx, :, :])**2))\r\n",
        "        #print(\"patch: {}\".format(idx))\r\n",
        "    \r\n",
        "    ############# NLM\r\n",
        "    next_state = next_patch\r\n",
        "    ### how to stitch  256x 256 patches into a single image fimg  after filtering\r\n",
        "    next_img = np.reshape(next_patch[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\r\n",
        "    current_image = np.reshape(state[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\r\n",
        "    \r\n",
        "    dist1img = current_image - GroundTruth\r\n",
        "    dist2img = next_img - GroundTruth  \r\n",
        "    dist2 = np.reshape(dist2img, (Patch_num), order='A')\r\n",
        "    dist1imgLarge = np.zeros((RESOLUTION+PATCH_reward-1,RESOLUTION+PATCH_reward-1))\r\n",
        "    margin = int((PATCH_reward-1)/2)\r\n",
        "    dist1imgLarge[margin:RESOLUTION+margin,margin:RESOLUTION+margin]=np.absolute(dist1img)\r\n",
        "    dist2imgLarge = np.zeros((RESOLUTION + PATCH_reward-1, RESOLUTION + PATCH_reward-1))\r\n",
        "    dist2imgLarge[margin:RESOLUTION + margin, margin:RESOLUTION + margin] = np.absolute(dist2img)\r\n",
        "    rewardimg = np.zeros((RESOLUTION,RESOLUTION))\r\n",
        "    \r\n",
        "    for i in range(RESOLUTION):\r\n",
        "        for j in range(RESOLUTION):\r\n",
        "\r\n",
        "            rewardimg[i,j]= 1/(np.sum(dist2imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.00001) - 1/(np.sum(dist1imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.00001)\r\n",
        "    reward = np.reshape(rewardimg,(Patch_num),order='A')\r\n",
        "\r\n",
        "            \r\n",
        "    \r\n",
        "    return next_state, reward, parameter_value, next_img\r\n",
        "\r\n",
        "\r\n",
        "def main():\r\n",
        "\r\n",
        "\r\n",
        "    #drive.mount('/content/drive')\r\n",
        "    \"\"\"\r\n",
        "    path = \"drive/My Drive/Colab Notebooks/LDCT/L004/Full dose images/*.dcm\"\r\n",
        "    TrueImgTrain = np.zeros((97,256,256))\r\n",
        "    i =0\r\n",
        "    for f in glob.glob(path):\r\n",
        "      ds = pydicom.dcmread(f)\r\n",
        "      data = resize(ds.pixel_array/4095.0,(256,256),anti_aliasing=False)\r\n",
        "      TrueImgTrain[i,:,:] =data    \r\n",
        "      i+=1\r\n",
        "    path = \"drive/My Drive/Colab Notebooks/LDCT/L004/Low Dose Images/*.dcm\"\r\n",
        "    train_data = np.zeros((97,256,256))\r\n",
        "    i =0\r\n",
        "    for f in glob.glob(path):\r\n",
        "      ds = pydicom.dcmread(f)\r\n",
        "      data = resize(ds.pixel_array/4095.0,(256,256),anti_aliasing=False)\r\n",
        "      train_data[i,:,:] =data\r\n",
        "      i+=1\r\n",
        "    \r\n",
        "    np.save(\"drive/My Drive/Colab Notebooks/LDCT/L004/full\",TrueImgTrain)\r\n",
        "    np.save(\"drive/My Drive/Colab Notebooks/LDCT/L004/low\",train_data)\r\n",
        "    \"\"\"\r\n",
        "    TrueImgTrain =np.load( \"C:/Users/ZhenjuYin/Downloads/LDCT-and-Projection-data/C016/full.npy\",allow_pickle=True)\r\n",
        "    train_data= np.load(\"C:/Users/ZhenjuYin/Downloads/LDCT-and-Projection-data/C016/low.npy\",allow_pickle=True)\r\n",
        "    TrueImgTrain = TrueImgTrain[0:1,:,:]\r\n",
        "    train_data = train_data[0:1,:,:]\r\n",
        "    datasize = TrueImgTrain.shape\r\n",
        "    \r\n",
        "    plt.set_cmap(plt.gray())\r\n",
        "\r\n",
        "    # (None, channel, H, w, depth) for Volume\r\n",
        "    mainDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1], len(Actions)).cuda().double()\r\n",
        "    targetDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1],len(Actions)).cuda().double()\r\n",
        "\r\n",
        "    state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\r\n",
        "    next_state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\r\n",
        "    action_sel = np.zeros((REPLAY_MEMORY))\r\n",
        "    reward_sel = np.zeros((REPLAY_MEMORY))\r\n",
        "  \r\n",
        "    \r\n",
        "    indicator = 0 \r\n",
        "    if MAX_EPOCHS>0:  ########################## len(Para),len(Para),\r\n",
        "                   \r\n",
        "            ##  slabs  x  256**2 x 9 x 9\r\n",
        "            \r\n",
        "            ############ State_initial shouldnt be changed !!!!!\r\n",
        "            State_initial = np.zeros((datasize[0], Patch_num, PATCH_SIZE[0],PATCH_SIZE[0])) \r\n",
        "            org = np.zeros((datasize[0], Patch_num, PATCH_SIZE[0],PATCH_SIZE[0]))\r\n",
        "            parameter_value = 0.5*np.ones((Patch_num,len(Para)))\r\n",
        "        \r\n",
        "            #parameter_value = np.load(\"drive/My Drive/Colab Notebooks/LDCT/C016/parameter.npy\",allow_pickle=True)\r\n",
        "            # for each image initializer\r\n",
        "            for IMG in range(datasize[0]):\r\n",
        "                state = divide_patch(  train_data[ IMG,:,:]  )  ##  256**2 x 9 x 9              \r\n",
        "                GroundTruth = TrueImgTrain[IMG,:, : ] \r\n",
        "                org[ IMG,:,:,:] = state\r\n",
        "                    \r\n",
        "                ## initialize the 1st and 2nd paths \r\n",
        "                \r\n",
        "                action = 2 * np.ones((Patch_num))\r\n",
        "                next_state, reward, parameter_value, img = Denoise( state,  action, parameter_value , GroundTruth, org[ IMG,:,:,:] )\r\n",
        "                #skimage.external.tifffile.imshow(img)                \r\n",
        "                State_initial[ IMG, :, :, :] = next_state\r\n",
        "                #print(IMG)\r\n",
        "\r\n",
        "            count_memory = 0\r\n",
        "\r\n",
        "            for episode in range(MAX_EPOCHS-1):\r\n",
        "                State = np.zeros((datasize[0], Patch_num, PATCH_SIZE[0],PATCH_SIZE[0])) \r\n",
        "                e = 0.999 / ((episode / 50) + 1)\r\n",
        "                if e<0.1:\r\n",
        "                    e=0.1\r\n",
        "                step_count = 0\r\n",
        "                State[:,:,:,:]  = State_initial[:,:,:,:] \r\n",
        "                parameter_value = 0.5*np.ones((Patch_num,len(Para)))\r\n",
        "                #print(State[0,25700,3,3])\r\n",
        "                for ITER_NUM in range(MAXSTEPS_FILTER):  \r\n",
        "                  \r\n",
        "                    for IMG_IDX in range(datasize[0]):\r\n",
        "                       \r\n",
        "                        state = State[ IMG_IDX, :, :, :]     ##  slabs x num of patches x 9 x 9\r\n",
        "                        GroundTruth = TrueImgTrain[IMG_IDX,:, : ]                      \r\n",
        "                        action = 2 * np.ones((Patch_num))\r\n",
        "\r\n",
        "                        # random select patches and     action  for each image\r\n",
        "                        flag = np.random.rand(Patch_num)\r\n",
        "                        count_patch = 0\r\n",
        "                        length_patch = 0\r\n",
        "                        for idx in range(Patch_num):\r\n",
        "                            if flag[idx]>=e:\r\n",
        "                                length_patch += 1\r\n",
        "                        \r\n",
        "                        # yy  : patch samples\r\n",
        "                        print(\"start sampling patches\")\r\n",
        "                        yy = torch.zeros(length_patch, 1, PATCH_SIZE[0],PATCH_SIZE[0], dtype=torch.float64)\r\n",
        "                        for idx in range(Patch_num):\r\n",
        "                            if flag[idx]<e:\r\n",
        "                                action[idx] = np.random.randint(len(Actions), size=1)\r\n",
        "                               \r\n",
        "                            if flag[idx]>=e:\r\n",
        "                                yy[count_patch,0,:, :] = torch.from_numpy(state[idx,:,:])\r\n",
        "                                count_patch += 1\r\n",
        "                        #print(yy.shape)\r\n",
        "                        y = mainDQN(Variable(yy).cuda())\r\n",
        "                       \r\n",
        "                        action_yy = torch.argmax(y, axis=1)\r\n",
        "                     \r\n",
        "                        #### action and paramter chosen\r\n",
        "                        count_patch=0\r\n",
        "                        for idx in range(Patch_num):\r\n",
        "                            if flag[idx] >= e:\r\n",
        "                                action[idx] = action_yy[count_patch]\r\n",
        "                               \r\n",
        "                                count_patch += 1\r\n",
        "                        print(\"strat denoise\")\r\n",
        "                        next_state, reward, parameter_value, img = Denoise( state,   action,parameter_value, GroundTruth, org[ IMG_IDX,:,:,:] )\r\n",
        "                        psnr =PSNR(GroundTruth, img )\r\n",
        "                        \r\n",
        "                        img1 = torch.zeros(1, 1, datasize[1],datasize[2],dtype=torch.float64)\r\n",
        "                        img2 = torch.zeros(1, 1, datasize[1],datasize[2],dtype=torch.float64)\r\n",
        "                        img1[:,0,:,:] = torch.from_numpy(GroundTruth)\r\n",
        "                        img2[:,0,:,:] = torch.from_numpy(img)\r\n",
        "                        ssim_ = ssim(img1,img2)\r\n",
        "                        \r\n",
        "                        if  ITER_NUM %10==0 :\r\n",
        "                          fig, axs = plt.subplots(1, 1,figsize=(10,10))\r\n",
        "                          fig.suptitle(\" iteration: {}\".format(ITER_NUM ))\r\n",
        "                          axs.imshow(img, cmap='gray')\r\n",
        "                          plt.show()\r\n",
        "                        \r\n",
        "                        print(\" current PSNR : {} ssim : {} reward:{} \".format(psnr,ssim_,np.mean(reward)))\r\n",
        "                        \r\n",
        "                        ###################  ? random replacement\r\n",
        "                        sel_prob = 0.01\r\n",
        "                        flag1 = np.random.rand(Patch_num)\r\n",
        "                        flag2 = np.zeros([Patch_num])\r\n",
        "                        for idx in range(Patch_num):\r\n",
        "                            if flag1[idx]>=sel_prob:\r\n",
        "                                flag2[idx] = 0\r\n",
        "                            if flag1[idx]<sel_prob:  ##### chosen\r\n",
        "                                flag2[idx] = 1\r\n",
        "\r\n",
        "                        sel_num = int(np.sum(flag2))\r\n",
        "\r\n",
        "                        ##### refresh the buffer randomly\r\n",
        "                        if count_memory+sel_num<=REPLAY_MEMORY-2:\r\n",
        "                            for idx in range(Patch_num):\r\n",
        "                                if flag1[idx]<sel_prob:\r\n",
        "                                    state_sel[count_memory,:,:] = state[idx,:,:]\r\n",
        "                                    next_state_sel[count_memory,:,:] = next_state[idx,:,:]\r\n",
        "                                    action_sel[count_memory]=action[idx]\r\n",
        "                                    reward_sel[count_memory] = reward[idx]\r\n",
        "                                    \r\n",
        "                                    count_memory += 1\r\n",
        "                        else:\r\n",
        "                            indicator = 1\r\n",
        "                            for idx in range(Patch_num):\r\n",
        "                                if flag1[idx]<sel_prob:\r\n",
        "                                    state_sel[count_memory,:] = state[idx,:,:]\r\n",
        "                                    next_state_sel[count_memory,:] = next_state[idx,:,:]\r\n",
        "                                    action_sel[count_memory]=action[idx]\r\n",
        "                                    reward_sel[count_memory] = reward[idx]\r\n",
        "                                    \r\n",
        "                                    if count_memory == REPLAY_MEMORY - 1:\r\n",
        "                                        count_memory = 0\r\n",
        "                                        print('Replay Memory is full')\r\n",
        "                                    else:\r\n",
        "                                        count_memory += 1\r\n",
        "                        if indicator == 0:\r\n",
        "                            replay_size = count_memory +  1\r\n",
        "                        else:\r\n",
        "                            replay_size = REPLAY_MEMORY\r\n",
        "\r\n",
        "                        if replay_size > BATCH_SIZE:\r\n",
        "\r\n",
        "                            #print(\" tuning\")\r\n",
        "                            \r\n",
        "                            for i in range(TUNING_STEP):\r\n",
        "                                shuffle_order = np.arange(replay_size)\r\n",
        "                                np.random.shuffle(shuffle_order)\r\n",
        "                                minibatch_state = state_sel[shuffle_order[0:BATCH_SIZE], :, :]\r\n",
        "                                minibatch_next_state = next_state_sel[shuffle_order[0:BATCH_SIZE],:,:]\r\n",
        "                                minibatch_action = action_sel[shuffle_order[0:BATCH_SIZE]]\r\n",
        "                                minibatch_reward = reward_sel[shuffle_order[0:BATCH_SIZE]]\r\n",
        "                                \r\n",
        "                                loss = replay_train(mainDQN, targetDQN, minibatch_state,minibatch_next_state,minibatch_action, minibatch_reward)\r\n",
        "                                if step_count % TARGET_UPDATE_STEP == 0:\r\n",
        "                                    targetDQN = copy.deepcopy(mainDQN)\r\n",
        "                                step_count += 1\r\n",
        "\r\n",
        "                        ###################   randomly store patches\r\n",
        "                        \"\"\"\r\n",
        "                        shuffle_order = np.arange(Patch_num)\r\n",
        "                        np.random.shuffle(shuffle_order)\r\n",
        "\r\n",
        "                        ##### refresh the buffer randomly\r\n",
        "                        if count_memory+Nsample<=REPLAY_MEMORY-2:\r\n",
        "                                             \r\n",
        "                          state_sel[count_memory:count_memory+Nsample,:,:] = state[shuffle_order[0:Nsample],:,:]\r\n",
        "                          next_state_sel[count_memory:count_memory+Nsample,:,:] = next_state[shuffle_order[0:Nsample],:,:]\r\n",
        "                          action_sel[count_memory:count_memory+Nsample]=action[shuffle_order[0:Nsample]]\r\n",
        "                         \r\n",
        "                          reward_sel[count_memory:count_memory+Nsample] = reward[shuffle_order[0:Nsample]]\r\n",
        "                    \r\n",
        "                          count_memory += Nsample\r\n",
        "                        else:\r\n",
        "                          indicator = 1\r\n",
        "                          remain = REPLAY_MEMORY- count_memory\r\n",
        "                          state_sel[count_memory:count_memory+remain,:,:] = state[shuffle_order[0:remain],:,:]\r\n",
        "                          next_state_sel[count_memory:count_memory+remain,:,:] = next_state[shuffle_order[0:remain],:,:]\r\n",
        "                          action_sel[count_memory:count_memory+remain]=action[shuffle_order[0:remain]]\r\n",
        "                          \r\n",
        "                          reward_sel[count_memory:count_memory+remain] = reward[shuffle_order[0:remain]]\r\n",
        "                          print('Replay Memory is full')\r\n",
        "                          count_memory = 0\r\n",
        "                          remain_ = Nsample- remain\r\n",
        "                          state_sel[count_memory:count_memory+remain_,:,:] = state[shuffle_order[remain:Nsample],:,:]\r\n",
        "                          next_state_sel[count_memory:count_memory+remain_,:,:] = next_state[shuffle_order[remain:Nsample],:,:]\r\n",
        "                          action_sel[count_memory:count_memory+remain_]=action[shuffle_order[remain:Nsample]]\r\n",
        "                          \r\n",
        "                          reward_sel[count_memory:count_memory+remain_] = reward[shuffle_order[remain:Nsample]]\r\n",
        "                        if indicator == 0:\r\n",
        "                            replay_size = count_memory +  1\r\n",
        "                        else:\r\n",
        "                            replay_size = REPLAY_MEMORY\r\n",
        "\r\n",
        "                        if replay_size > BATCH_SIZE:\r\n",
        "\r\n",
        "                            print(\" tuning\")\r\n",
        "                            \r\n",
        "                            for i in range(TUNING_STEP):\r\n",
        "                                shuffle_order = np.arange(replay_size)\r\n",
        "                                np.random.shuffle(shuffle_order)\r\n",
        "                                minibatch_state = state_sel[shuffle_order[0:BATCH_SIZE], :, :]\r\n",
        "                                minibatch_next_state = next_state_sel[shuffle_order[0:BATCH_SIZE],:,:]\r\n",
        "                                minibatch_action = action_sel[shuffle_order[0:BATCH_SIZE]]\r\n",
        "                               \r\n",
        "                                minibatch_reward = reward_sel[shuffle_order[0:BATCH_SIZE]]\r\n",
        "                                \r\n",
        "                                loss = replay_train(mainDQN, targetDQN, minibatch_state,minibatch_next_state,minibatch_action,minibatch_reward)\r\n",
        "                                if step_count % TARGET_UPDATE_STEP == 0:\r\n",
        "                                    targetDQN = copy.deepcopy(mainDQN)\r\n",
        "                                step_count += 1\r\n",
        "                                #print(step_count)\r\n",
        "                        \"\"\"\r\n",
        "                        State[IMG_IDX, :, :, :] = next_state\r\n",
        "                        #np.save(\"drive/My Drive/Colab Notebooks/LDCT/C016/parameter\",parameter_value)\r\n",
        "                        \r\n",
        "\r\n",
        "                    print(\"Episode: {}  Iteration: {} Loss: {}\".format(episode, ITER_NUM, loss))\r\n",
        "\r\n",
        "                CHECK = episode+1\r\n",
        "    # save the trained networks\r\n",
        "    PATH = \"C:/Users/ZhenjuYin/Downloads/LDCT-and-Projection-data/mainDQN\"\r\n",
        "    torch.save(mainDQN.state_dict(), PATH)\r\n",
        "    PATH = \"C:/Users/ZhenjuYin/Downloads/LDCT-and-Projection-data//targetDQN\"\r\n",
        "    torch.save(targetDQN.state_dict(), PATH)\r\n",
        "    PATH = \"C:/Users/ZhenjuYin/Downloads/LDCT-and-Projection-data/parameter_value\"\r\n",
        "    np.save(PATH,parameter_value)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}