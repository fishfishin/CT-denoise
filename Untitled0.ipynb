{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMSsKbnU81u0YHNkjNGngK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fishfishin/CT-denoise/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NZFTkN1Yk6l",
        "outputId": "e5867ca9-d53a-491b-f250-c8dc1684673d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting visdom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n",
            "\u001b[K     |████████████████████████████████| 686kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from visdom) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom) (19.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/d0/34b0f59ac08de9c1e07876cfecd80aec650600177b4bd445124c755499a7/jsonpatch-1.26-py2.py3-none-any.whl\n",
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 12.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom) (7.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (3.0.4)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655251 sha256=090c68f06f24b0d8934600c55d4632a7eb40673a4e5c38cf71c6e6ac0383201e\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=c5ccf6a97fef8a56c7f2777ba1443110fb0c2f7237048d29777b9f3e54a06176\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n",
            "Successfully installed jsonpatch-1.26 jsonpointer-2.0 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5oOrOBAZGPQ",
        "outputId": "cc6a4bce-7261-423e-fea9-b34e38ad65f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import visdom\n",
        "import torchvision.transforms as T\n",
        "import skimage.external.tifffile\n",
        "import copy\n",
        "import cv2\n",
        "import math\n",
        "from skimage.restoration import denoise_nl_means\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():  \n",
        "  dev = \"cuda:0\" \n",
        "else:  \n",
        "  dev = \"cpu\"  \n",
        "print(dev)\n",
        "#vis = visdom.Visdom()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD5qj_ttAazJ"
      },
      "source": [
        "class doubleDQN(nn.Module):\n",
        "\n",
        "    def __init__(self, h, w, output1,output2):\n",
        "        super(doubleDQN, self).__init__()\n",
        "\n",
        "        # image patch size is   h x w\n",
        "        #first two  shared conv layers\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=(2),padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=(2),padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # patch size after conv layer\n",
        "        def conv2d_size_out(size, kernel_size = 3, stride = 2):\n",
        "            return size // stride  + 1\n",
        "        convw = conv2d_size_out(conv2d_size_out(w))\n",
        "        convh = conv2d_size_out(conv2d_size_out(h))\n",
        "\n",
        "        linear_input_size = convw * convh * 64\n",
        "\n",
        "        #  Parameter selection\n",
        "        #  conv 64 Filter, kernel 3x3 \n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        #  FCL 128 neurons\n",
        "        self.dense1 = nn.Linear(linear_input_size, 128)\n",
        "        self.bn4 = nn.BatchNorm1d(128)\n",
        "        self.head1 = nn.Linear(128,output1)\n",
        "       \n",
        "        linear_input_size = convw * convh  * 128\n",
        "        #  Parameter tuning\n",
        "        #  conv 128 Filter, kernel 3x3 \n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        #  conv 128 Filter, kernel 3x3 \n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1,padding=1)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "        #  FCL 256 neurons\n",
        "        self.dense2 = nn.Linear(linear_input_size, 256)\n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        self.head2 = nn.Linear(256, output2)\n",
        "\n",
        "        self.opt = optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "\n",
        "        # 2 shared layers\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "        #  Parameter selection\n",
        "        out1 = F.relu(self.bn3(self.conv3(x)))\n",
        "        out1 = out1.view(out1.size(0), -1)\n",
        "        out1 = F.relu(self.bn4(self.dense1(out1)))\n",
        "        out1 = self.head1(out1)\n",
        "       \n",
        "        #  Parameter tuning\n",
        "        out2 = F.relu(self.bn5(self.conv4(x)))\n",
        "        out2 = F.relu(self.bn6(self.conv5(out2)))\n",
        "        out2 = out2.view(out2.size(0), -1)\n",
        "        out2 = F.relu(self.bn7(self.dense2(out2)))\n",
        "        out2 = self.head2(out2)\n",
        "\n",
        "        return  out1, out2\n",
        "\n",
        "\n",
        "    def update(self, x,target1,target2):\n",
        "\n",
        "        out1,out2 = self.forward(x)\n",
        "        self.opt.zero_grad()\n",
        "        \n",
        "        loss1 = nn.MSELoss()\n",
        "        loss2 = nn.MSELoss()\n",
        "        upd_1 = loss1(out1,target1 )\n",
        "        upd_2 = loss2(out2,target2 )\n",
        "        upd_ = upd_1 + upd_2\n",
        "        upd_.backward()\n",
        "        \n",
        "        self.opt.step()\n",
        "\n",
        "        return upd_"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFyfDzO7ZQuK"
      },
      "source": [
        "\n",
        "\n",
        "####  1st path\n",
        "Para = ['sigma', 'size', 'dist']\n",
        "\n",
        "####  2nd path\n",
        "Actions = ['plus_half', 'plus_tenth', 'null', 'minus_tenth', 'minus_half']\n",
        "\n",
        "MAX_EPOCHS = 10\n",
        "DISCOUNT_RATE = 0.99\n",
        "RESOLUTION = 256\n",
        "PATCH_SIZE = [9,9]\n",
        "Patch_num = RESOLUTION **2\n",
        "PATCH_reward = 5\n",
        "TARGET_UPDATE_STEP = 300\n",
        "MAXSTEPS_FILTER= 30\n",
        "REPLAY_MEMORY = 3200  ######### buffer？\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "TUNING_STEP= 20\n",
        "\n",
        "def replay_train(mainDQN: doubleDQN, targetDQN: doubleDQN, states, next_states, action,parameter, rewards) -> np.float64:\n",
        "\n",
        "    X = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0])\n",
        "    X1 = torch.zeros(states.shape[0], 1, PATCH_SIZE[0],PATCH_SIZE[0])\n",
        "    X[:,0,:,:] = torch.from_numpy(states)\n",
        "    X1[:,0,:,:] = torch.from_numpy(next_states)\n",
        "\n",
        "    t1, t2 = targetDQN(X1)  #### old version of NET\n",
        "    y1, y2 = mainDQN(X)\n",
        "\n",
        "    ####  1st path\n",
        "    tem = torch.max(t1, axis=1)\n",
        "    temp = tem.values\n",
        "    Q_target1 = rewards + DISCOUNT_RATE * temp.detach().numpy()\n",
        "    for i in range(y1.shape[0]):\n",
        "        y1[i,int(parameter[i])] = Q_target1[i]\n",
        "\n",
        "    ####  2nd path\n",
        "    tem = torch.max(t2, axis=1)\n",
        "    temp = tem.values\n",
        "    Q_target2 = rewards + DISCOUNT_RATE * temp.detach().numpy()\n",
        "    for i in range(y2.shape[0]):\n",
        "        y2[i,int(action[i])] = Q_target2[i]\n",
        "\n",
        "    return mainDQN.update(X, y1,y2)\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34-UqzF-Za8j"
      },
      "source": [
        "def PSNR(img1, img2):\n",
        "    D = np.array(img1 - img2, dtype=np.float64)\n",
        "    D[:, :] = D[:, :]**2\n",
        "    RMSE = D.sum()/img1.size\n",
        "    psnr = 10*math.log10(float(255.**2)/RMSE)\n",
        "    return psnr"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cv3BPJ0Zu5E"
      },
      "source": [
        "def divide_patch(fimg) -> np.float64:\n",
        "\n",
        "    fimgpad = np.zeros((RESOLUTION + PATCH_SIZE[0] -1, RESOLUTION + PATCH_SIZE[0] -1),dtype =np.float64)\n",
        "    fimgpad[int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1,int((PATCH_SIZE[0]+1)/2)-1:RESOLUTION+int((PATCH_SIZE[0]+1)/2)-1]=fimg\n",
        "    state = np.zeros((Patch_num,PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    count = 0\n",
        "    for xcord in range(RESOLUTION):\n",
        "        for ycord in range(RESOLUTION):\n",
        "            temp=fimgpad[xcord:xcord+PATCH_SIZE[0],ycord:ycord+PATCH_SIZE[0]]\n",
        "            state[count,:,: ] = temp\n",
        "            count += 1\n",
        "    return state\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLLD25fKZyhj"
      },
      "source": [
        "def Denoise(state, parameter, action, parameter_value, GroundTruth, original_image) -> np.float64:\n",
        "\n",
        "    current_patch = np.zeros((Patch_num, PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    org_patch = divide_patch(original_image)\n",
        "    # tuning parameter\n",
        "    for idx in range(Patch_num):\n",
        "        if parameter[idx] ==0:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,0] = parameter_value[idx,0] *1.5\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,0] = parameter_value[idx,0] *1.1\n",
        "            if action[idx]==3:\n",
        "                parameter_value[idx,0] = parameter_value[idx,0]*0.9\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,0]= parameter_value[idx,0] *0.5\n",
        "        if parameter[idx]==1:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *1.5\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *1.1\n",
        "            if action[idx]==3:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1]*0.9\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,1] = parameter_value[idx,1] *0.5\n",
        "        if parameter[idx]==2:\n",
        "            if action[idx]==0:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *1.5\n",
        "            if action[idx]==1:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *1.1\n",
        "            if action[idx]==3:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1]*0.9\n",
        "            if action[idx]==4:\n",
        "                parameter_value[idx,2] = parameter_value[idx,1] *0.5\n",
        "\n",
        "        final_patch = denoise_nl_means(org_patch[idx,:,:], h =1.15*parameter_value[idx,0],fast_mode=True,patch_size=parameter_value[idx,1],\n",
        "                                                          patch_distance=parameter_value[idx,2],multichannel=False)\n",
        "        current_patch [idx, :, :] = final_patch\n",
        "       \n",
        "    ############# NLM\n",
        "\n",
        "    ### how to stitch  256x 256 patches into a single image fimg  after Bm3d\n",
        "    next_img = np.reshape(current_patch[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\n",
        "    next_state = current_patch\n",
        "    current_image = np.reshape(state[:, int(PATCH_SIZE[0]//2), int(PATCH_SIZE[0]//2)], (RESOLUTION, RESOLUTION), order='A')\n",
        "    #############   calculate reward and error\n",
        "\n",
        "    dist1img = current_image - GroundTruth\n",
        "    dist2img = next_img - GroundTruth  #######################################\n",
        "    dist2 = np.reshape(dist2img, (Patch_num), order='A')\n",
        "\n",
        "    dist1imgLarge = np.zeros((RESOLUTION+PATCH_reward-1,RESOLUTION+PATCH_reward-1))\n",
        "    margin = int((PATCH_reward-1)/2)\n",
        "    dist1imgLarge[margin:RESOLUTION+margin,margin:RESOLUTION+margin]=np.absolute(dist1img)\n",
        "\n",
        "    dist2imgLarge = np.zeros((RESOLUTION + PATCH_reward-1, RESOLUTION + PATCH_reward-1))\n",
        "    dist2imgLarge[margin:RESOLUTION + margin, margin:RESOLUTION + margin] = np.absolute(dist2img)\n",
        "\n",
        "    rewardimg = np.zeros((RESOLUTION,RESOLUTION))\n",
        "    \n",
        "    for i in range(RESOLUTION):\n",
        "        for j in range(RESOLUTION):\n",
        "           \n",
        "            rewardimg[i,j]= 1/(np.sum(dist2imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.001) - 1/(np.sum(dist1imgLarge[i:i+PATCH_reward,j:j+PATCH_reward])+0.001)\n",
        "    reward = np.reshape(rewardimg,(Patch_num),order='A')\n",
        "    error = np.sum(np.absolute(dist2))\n",
        "\n",
        "\n",
        "    return next_state, reward, parameter_value, next_img ,error"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqkRdY-uadPN",
        "outputId": "8b906883-e729-4171-c713-aba78b6c6d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def main():\n",
        "\n",
        "    #train_data = skimage.external.tifffile.imread(\"C:/Users/ZhenjuYin/Downloads/a1.tif\")\n",
        "    drive.mount('/content/drive')\n",
        "    TrueImgTrain = skimage.external.tifffile.imread(\"drive/My Drive/Colab Notebooks/G.tif\")\n",
        "    print(TrueImgTrain.shape)\n",
        "    #TrueImgTrain = TrueImgTrain[:,:,:]\n",
        "    datasize = TrueImgTrain.shape\n",
        "    gaussian = np.random.normal(0.3, 0.9, (datasize[0],datasize[1],datasize[2])) \n",
        "    train_data = TrueImgTrain + gaussian\n",
        "    \n",
        "    # (None, channel, H, w, depth) for Volume\n",
        "    mainDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1], len(Para),len(Actions))\n",
        "    targetDQN = doubleDQN(PATCH_SIZE[0], PATCH_SIZE[1], len(Para),len(Actions))\n",
        "\n",
        "    state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    next_state_sel = np.zeros((REPLAY_MEMORY, PATCH_SIZE[0], PATCH_SIZE[0]))\n",
        "    action_sel = np.zeros((REPLAY_MEMORY))\n",
        "    reward_sel = np.zeros((REPLAY_MEMORY))\n",
        "    para_sel = np.zeros((REPLAY_MEMORY))\n",
        "    \n",
        "    indicator = 0 \n",
        "    if MAX_EPOCHS>0:  ########################## \n",
        "                   \n",
        "            State = np.zeros((datasize[0], Patch_num, PATCH_SIZE[0],PATCH_SIZE[0]))             ##  slabs  x  256**2 x 9 x 9\n",
        "            parameter_value = 0.99 * np.ones((Patch_num, len(Para)))\n",
        "             \n",
        "            # for each image initializer\n",
        "            for IMG in range(datasize[0]):\n",
        "                state = divide_patch(  train_data[ IMG,:,:]  )  ##  256**2 x 9 x 9              \n",
        "                GroundTruth = TrueImgTrain[IMG,:, : ]      \n",
        "                ## initialize the 1st and 2nd paths \n",
        "                parameter = np.ones((Patch_num))\n",
        "                action = 2 * np.ones((Patch_num))\n",
        "                next_state, reward, parameter_value, img, error = Denoise( state,  parameter, action, parameter_value , GroundTruth, train_data[ IMG,:,:] )\n",
        "                State[ IMG, :, :, :] = next_state\n",
        "                \n",
        "                print(IMG)\n",
        "\n",
        "\n",
        "            State_initial = State\n",
        "            count_memory = 0\n",
        "\n",
        "            for episode in range(MAX_EPOCHS-1):\n",
        "\n",
        "                e = 0.999 / ((episode / 150) + 1)\n",
        "                if e<0.1:\n",
        "                    e=0.1\n",
        "                step_count = 0\n",
        "                State = State_initial\n",
        "\n",
        "                for ITER_NUM in range(MAXSTEPS_FILTER):\n",
        "\n",
        "                    for IMG_IDX in range(datasize[0]):\n",
        "                       \n",
        "                        state = State[ IMG_IDX, :, :, :]     ##  slabs x num of patches x 9 x 9\n",
        "                        GroundTruth = TrueImgTrain[IMG_IDX,:, : ]                      \n",
        "                        parameter = np.ones((Patch_num))\n",
        "                        action = 2 * np.ones((Patch_num))\n",
        "\n",
        "                        # random select patches and     action  for each image\n",
        "                        flag = np.random.rand(Patch_num)\n",
        "                        count_patch = 0\n",
        "                        length_patch = 0\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx]>=e:\n",
        "                                length_patch += 1\n",
        "                        \n",
        "                        # yy  : patch samples\n",
        "                        yy = torch.zeros(length_patch, 1, PATCH_SIZE[0],PATCH_SIZE[0])\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx]<e:\n",
        "                                action[idx] = np.random.randint(len(Actions), size=1)\n",
        "                                parameter[idx] = np.random.randint(len(Para), size=1)\n",
        "                            if flag[idx]>=e:\n",
        "                                yy[count_patch,0,:, :] = torch.from_numpy(state[idx,:,:])\n",
        "                                count_patch += 1\n",
        "                        \n",
        "                        y1, y2 = mainDQN(yy)\n",
        "                        parameter_yy = torch.argmax(y1, axis=1)\n",
        "                        action_yy = torch.argmax(y2, axis=1)\n",
        "                     \n",
        "                        #### action and paramter chosen\n",
        "                        count_patch=0\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag[idx] >= e:\n",
        "                                action[idx] = action_yy[count_patch]\n",
        "                                parameter[idx] = parameter_yy[count_patch]\n",
        "                                count_patch += 1\n",
        "                        \n",
        "                        next_state, reward, parameter_value, img, error = Denoise( state,  parameter, action,parameter_value,  GroundTruth, train_data[ IMG_IDX,:,:] )\n",
        "                        psnr =PSNR(train_data[IMG_IDX,:,:], img )\n",
        "                        print(\" current PSNR : {}\".format(psnr))\n",
        "                        name = str(step_count)\n",
        "                        #vis.image( img,opts='store_history')\n",
        "                        \n",
        "                        ###################  ? random replacement\n",
        "                        sel_prob = 0.01\n",
        "                        flag1 = np.random.rand(Patch_num)\n",
        "                        flag2 = np.zeros([Patch_num])\n",
        "                        for idx in range(Patch_num):\n",
        "                            if flag1[idx]>=sel_prob:\n",
        "                                flag2[idx] = 0\n",
        "                            if flag1[idx]<sel_prob:  ##### chosen\n",
        "                                flag2[idx] = 1\n",
        "\n",
        "                        sel_num = int(np.sum(flag2))\n",
        "\n",
        "                        ##### refresh the buffer randomly\n",
        "                        if count_memory+sel_num<=REPLAY_MEMORY-2:\n",
        "                            for idx in range(Patch_num):\n",
        "                                if flag1[idx]<sel_prob:\n",
        "                                    state_sel[count_memory,:,:] = state[idx,:,:]\n",
        "                                    next_state_sel[count_memory,:,:] = next_state[idx,:,:]\n",
        "                                    action_sel[count_memory]=action[idx]\n",
        "                                    para_sel[count_memory] = parameter[idx]\n",
        "                                    reward_sel[count_memory] = reward[idx]\n",
        "                                    #value_sel[count_memory] = parameter_value[idx,:]\n",
        "                                    \n",
        "                                    count_memory += 1\n",
        "                        else:\n",
        "                            indicator = 1\n",
        "                            for idx in range(Patch_num):\n",
        "                                if flag1[idx]<sel_prob:\n",
        "                                    state_sel[count_memory,:] = state[idx,:,:]\n",
        "                                    next_state_sel[count_memory,:] = next_state[idx,:,:]\n",
        "                                    action_sel[count_memory]=action[idx]\n",
        "                                    para_sel[count_memory] = parameter[idx]\n",
        "                                    reward_sel[count_memory] = reward[idx]\n",
        "                                    #value_sel[count_memory] = parameter_value[idx,:]\n",
        "                                    \n",
        "                                    if count_memory == REPLAY_MEMORY - 1:\n",
        "                                        count_memory = 0\n",
        "                                        print('Replay Memory is full')\n",
        "                                    else:\n",
        "                                        count_memory += 1\n",
        "                        if indicator == 0:\n",
        "                            replay_size = count_memory +  1\n",
        "                        else:\n",
        "                            replay_size = REPLAY_MEMORY\n",
        "\n",
        "                        if replay_size > BATCH_SIZE:\n",
        "\n",
        "                            print(\" tuning\")\n",
        "                            \n",
        "                            for i in range(TUNING_STEP):\n",
        "                                shuffle_order = np.arange(replay_size)\n",
        "                                np.random.shuffle(shuffle_order)\n",
        "                                minibatch_state = state_sel[shuffle_order[0:BATCH_SIZE], :, :]\n",
        "                                minibatch_next_state = next_state_sel[shuffle_order[0:BATCH_SIZE],:,:]\n",
        "                                minibatch_action = action_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                minibatch_parameter = para_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                minibatch_reward = reward_sel[shuffle_order[0:BATCH_SIZE]]\n",
        "                                \n",
        "                                loss = replay_train(mainDQN, targetDQN, minibatch_state,minibatch_next_state,minibatch_action,minibatch_parameter, minibatch_reward)\n",
        "                                if step_count % TARGET_UPDATE_STEP == 0:\n",
        "                                    targetDQN = copy.deepcopy(mainDQN)\n",
        "                                step_count += 1\n",
        "\n",
        "                        State[IMG_IDX, :, :, :] = next_state\n",
        "                        \n",
        "\n",
        "                    print(\"Episode: {}  Iterations: {} Loss: {}\".format(episode, ITER_NUM, loss))\n",
        "\n",
        "                CHECK = episode+1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "(16, 256, 256)\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            " current PSNR : 21.225290746928913\n",
            " tuning\n",
            " current PSNR : 19.048837088358535\n",
            " tuning\n",
            " current PSNR : 17.934098850526166\n",
            " tuning\n",
            " current PSNR : 17.312874370373795\n",
            " tuning\n",
            " current PSNR : 16.963958936495665\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 16.73910586507779\n",
            " tuning\n",
            " current PSNR : 16.71499245124449\n",
            " tuning\n",
            " current PSNR : 16.767741419281393\n",
            " tuning\n",
            " current PSNR : 16.772030101892575\n",
            " tuning\n",
            " current PSNR : 16.69806272288605\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 16.753006206182214\n",
            " tuning\n",
            " current PSNR : 16.714747716373303\n",
            " tuning\n",
            " current PSNR : 16.71428048216642\n",
            " tuning\n",
            " current PSNR : 16.989339608004318\n",
            " tuning\n",
            " current PSNR : 17.100372023933193\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 17.16459591050963\n",
            " tuning\n",
            "Episode: 0  Iterations: 0 Loss: 0.0052236407063901424\n",
            " current PSNR : 17.0710180188194\n",
            " tuning\n",
            " current PSNR : 17.03108608708035\n",
            " tuning\n",
            " current PSNR : 17.178547785289304\n",
            " tuning\n",
            " current PSNR : 17.30744095488287\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 17.37842584457972\n",
            " tuning\n",
            " current PSNR : 17.58388091495044\n",
            " tuning\n",
            " current PSNR : 17.546910115981827\n",
            " tuning\n",
            " current PSNR : 17.70034247288063\n",
            " tuning\n",
            " current PSNR : 17.61947312388729\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 17.64828685590401\n",
            " tuning\n",
            " current PSNR : 17.874080861902275\n",
            " tuning\n",
            " current PSNR : 18.097473609185506\n",
            " tuning\n",
            " current PSNR : 18.206372204891732\n",
            " tuning\n",
            " current PSNR : 18.300290181582923\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 18.493423476359713\n",
            " tuning\n",
            " current PSNR : 18.472526461877194\n",
            " tuning\n",
            "Episode: 0  Iterations: 1 Loss: 0.005229835398495197\n",
            " current PSNR : 18.169871669532768\n",
            " tuning\n",
            " current PSNR : 18.157897949935034\n",
            " tuning\n",
            " current PSNR : 17.965405608277912\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 17.9710185652078\n",
            " tuning\n",
            " current PSNR : 18.333713380443637\n",
            " tuning\n",
            " current PSNR : 18.312788050396957\n",
            " tuning\n",
            " current PSNR : 18.653270600377695\n",
            " tuning\n",
            " current PSNR : 18.888092904252932\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 18.965214605132026\n",
            " tuning\n",
            " current PSNR : 18.86563804105771\n",
            " tuning\n",
            " current PSNR : 18.732997186616796\n",
            " tuning\n",
            " current PSNR : 18.888192744855452\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 19.026423368873342\n",
            " tuning\n",
            " current PSNR : 19.172117348507555\n",
            " tuning\n",
            " current PSNR : 19.512281539861924\n",
            " tuning\n",
            " current PSNR : 19.66682388549534\n",
            " tuning\n",
            "Episode: 0  Iterations: 2 Loss: 0.004400552250444889\n",
            " current PSNR : 19.20414038305634\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 19.30408179381918\n",
            " tuning\n",
            " current PSNR : 19.236441038846436\n",
            " tuning\n",
            " current PSNR : 19.36469936523067\n",
            " tuning\n",
            " current PSNR : 19.46485413705197\n",
            " tuning\n",
            " current PSNR : 19.441103083933754\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 19.310903134284313\n",
            " tuning\n",
            " current PSNR : 19.484751629142444\n",
            " tuning\n",
            " current PSNR : 19.43432029554941\n",
            " tuning\n",
            " current PSNR : 19.553545751639017\n",
            " tuning\n",
            " current PSNR : 19.47984845491126\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 19.70844095054378\n",
            " tuning\n",
            " current PSNR : 19.9357106682901\n",
            " tuning\n",
            " current PSNR : 19.98422472685836\n",
            " tuning\n",
            " current PSNR : 19.832891158019994\n",
            " tuning\n",
            " current PSNR : 19.968880710471808\n",
            "Replay Memory is full\n",
            " tuning\n",
            "Episode: 0  Iterations: 3 Loss: 0.0061444914899766445\n",
            " current PSNR : 19.88478975478562\n",
            " tuning\n",
            " current PSNR : 19.851091690678743\n",
            " tuning\n",
            " current PSNR : 20.077925179389226\n",
            " tuning\n",
            " current PSNR : 20.306989206284655\n",
            " tuning\n",
            " current PSNR : 20.470656702552922\n",
            "Replay Memory is full\n",
            " tuning\n",
            " current PSNR : 20.679025133172964\n",
            " tuning\n",
            " current PSNR : 20.456353240107088\n",
            " tuning\n",
            " current PSNR : 20.239984644889667\n",
            " tuning\n",
            " current PSNR : 20.47256479256932\n",
            " tuning\n",
            " current PSNR : 20.409714691748007\n",
            "Replay Memory is full\n",
            " tuning\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}